{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fugue SQL\n",
    "\n",
    "It's strongly recommended to quickly go through the [COVID19 example](./example_covid19.ipynb) to get a sense of what Fugue SQL can do, and how it works. And here we are going through details of different Fugue SQL features.\n",
    "\n",
    "Fugue SQL is an alternative to Fugue programming interface. Both of them are used to describe your end to end workflow logic. The SQL sementic is platform and scale agnostic, so if you write logic in SQL, it's very high level and abstract, and the underlying computing frameworks will try to excute them in the optimal way. For example when you use `SparkExecutionEngine`, the sql statements are executed as SparkSQL, which is highly optimized for execution.\n",
    "\n",
    "The syntax of Fugue SQL is between standard SQL, json and python. The goals are\n",
    "* To be fully compatible with standard SQL `SELECT` statement\n",
    "* To minimize syntax overhead, to make code as short as possible while still easy to read\n",
    "\n",
    "## Hello World\n",
    "\n",
    "In order to use Fugue SQL, you firstly need to make sure you have installed the sql extra\n",
    "```\n",
    "pip install fugue[sql]\n",
    "```\n",
    "To use Fugue SQL in your program, you need to use `FugueSQLWorkflow` derived from `FugueWorkflow`, it has all the programming interface features plus Fugue SQL support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fugue_sql import FugueSQLWorkflow\n",
    "\n",
    "with FugueSQLWorkflow() as dag:\n",
    "    dag(\"\"\"\n",
    "    CREATE [[0,\"hello\"],[1,\"world\"]] SCHEMA a:int,b:str\n",
    "    PRINT\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the SQL will be translated to sequence of operations in programming interface, to construct the workflow. And you can mix with programming interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with FugueSQLWorkflow() as dag:\n",
    "    df = dag.df([[0,\"hello\"],[1,\"world\"]],\"a:int,b:str\")\n",
    "    dag(\"\"\"\n",
    "    SELECT * FROM df WHERE a=0  # see we can use df directly defined outside\n",
    "    PRINT\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use all local variables before the SQL call in the SQL. For dataframes, you can use them directly, for other variables, we use [JinjaSQL](https://github.com/hashedin/jinjasql) based on [Jinja2](https://jinja.palletsprojects.com/en/2.11.x/) for the template logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with FugueSQLWorkflow() as dag:\n",
    "    df = dag.df([[0,\"hello\"],[1,\"world\"]],\"a:int,b:str\")\n",
    "    x=0\n",
    "    dag(\"\"\"\n",
    "    SELECT * FROM df WHERE a={{x}}  # see we can use variable x directly\n",
    "    PRINT\n",
    "    \"\"\")\n",
    "    dag(\"\"\"\n",
    "    SELECT * FROM df WHERE a={{y}}  # or in this way\n",
    "    PRINT\n",
    "    \"\"\", y=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Local variables or previous SQL block defined variables can be used directly in the next SQL block. All variables (dataframes) defined in SQL blocks can be accessed by `dag[\"<key>\"]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with FugueSQLWorkflow() as dag:\n",
    "    dag(\"\"\"\n",
    "    a=CREATE [[0,\"hello\"],[1,\"world\"]] SCHEMA a:int,b:str\n",
    "    \"\"\")\n",
    "    dag(\"\"\"\n",
    "    PRINT a\n",
    "    \"\"\")\n",
    "    dag[\"a\"].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anonymity\n",
    "\n",
    "In Fugue SQL a very important simplification is anonymity, it's optional, but it usually can significantly simplify your code.\n",
    "\n",
    "For a statement that onlly needs to consume the previous dataframe, you can use anonymity. `PRINT` is the best example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fugue_sql import FugueSQLWorkflow\n",
    "\n",
    "with FugueSQLWorkflow() as dag:\n",
    "    dag(\"\"\"\n",
    "    a=CREATE [[0,\"hello\"],[1,\"world\"]] SCHEMA a:int,b:str\n",
    "    PRINT   # here if i don't specify, it means I want to print a -- the last dataframe output of the previous statements\n",
    "    PRINT   # I can use anonymity again because PRINT doesn't generate output, so it still means PRINT a\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For statements that don't generate output, you can't assign it to any variable. For statements that generates single output, you can also use anonymity and don't assign to a variable. The following statements will have to use anonymity if they need to consume this output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fugue_sql import FugueSQLWorkflow\n",
    "\n",
    "with FugueSQLWorkflow() as dag:\n",
    "    dag(\"\"\"\n",
    "    a=CREATE [[0,\"hello\"]] SCHEMA a:int,b:str\n",
    "    CREATE [[1,\"world\"]] SCHEMA a:int,b:str\n",
    "    PRINT     # print the second \n",
    "    PRINT a   # print the first, because it is explicit\n",
    "    PRINT     # print the second\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not only Fugue extensions can use anonymity, `SELECT` statement can also use it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with FugueSQLWorkflow() as dag:\n",
    "    dag(\"\"\"\n",
    "    CREATE [[0,\"hello\"], [1,\"world\"]] SCHEMA a:int,b:str\n",
    "    SELECT * WHERE a=1  # I don't need FROM, and it means FROM the last output of the previous statements\n",
    "    PRINT\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inline Statements\n",
    "\n",
    "You can make one statement inside another using `(` `)`. But since you can easily do variable assignment in Fugue, you may not need to couple your code in this way. It's all up to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with FugueSQLWorkflow() as dag:\n",
    "    dag(\"\"\"\n",
    "    SELECT * \n",
    "        FROM (CREATE [[0,\"hello\"], [1,\"world\"]] SCHEMA a:int,b:str)\n",
    "        WHERE a=1\n",
    "    PRINT\n",
    "    \"\"\")\n",
    "    dag(\"\"\"\n",
    "    PRINT (\n",
    "        SELECT * \n",
    "            FROM (CREATE [[0,\"hello\"], [1,\"world\"]] SCHEMA a:int,b:str)\n",
    "            WHERE a=1\n",
    "    )\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Data\n",
    "\n",
    "> **CREATE** array [**SCHEMA** schema]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with FugueSQLWorkflow() as dag:\n",
    "    dag(\"\"\"\n",
    "    CREATE [[0,\"hello\"],[1,\"world\"]] SCHEMA a:int,b:str\n",
    "    PRINT\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Creator\n",
    "\n",
    "> **CREATE USING** extension [(params)] [**SCHEMA** schema]\n",
    "\n",
    "Please also read [this](./creator.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Any\n",
    "import pandas as pd\n",
    "\n",
    "def create1(n=1) -> pd.DataFrame:\n",
    "    return pd.DataFrame([[n]],columns=[\"a\"])\n",
    "\n",
    "# schema: a:int\n",
    "def create2(n=1) -> List[List[Any]]:\n",
    "    return [[n]]\n",
    "\n",
    "def create3(n=1) -> List[List[Any]]:\n",
    "    return [[n]]\n",
    "\n",
    "with FugueSQLWorkflow() as dag:\n",
    "    dag(\"\"\"\n",
    "    CREATE USING create1 PRINT\n",
    "    CREATE USING create1(n=2) PRINT\n",
    "    \n",
    "    CREATE USING create2(n=3) PRINT\n",
    "    \n",
    "    CREATE USING create3(n=4) SCHEMA a:int PRINT\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data From File\n",
    "\n",
    "> **LOAD** [PARQUET|CSV|JSON] path [(params)] [**COLUMNS** schema|columns]\n",
    "\n",
    "Only if the path has no explicit suffix, you need to sepcify the file type hint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with FugueSQLWorkflow() as dag:\n",
    "    dag(\"\"\"\n",
    "    CREATE [[0,\"1\"]] SCHEMA a:int,b:str\n",
    "    SAVE OVERWRITE \"/tmp/f.parquet\"\n",
    "    SAVE OVERWRITE \"/tmp/f.csv\"(header=true)\n",
    "    SAVE OVERWRITE \"/tmp/f.json\"\n",
    "    SAVE OVERWRITE PARQUET \"/tmp/f\"\n",
    "    \"\"\")\n",
    "    dag(\"\"\"\n",
    "    LOAD \"/tmp/f.parquet\" PRINT\n",
    "    LOAD \"/tmp/f.parquet\" COLUMNS a PRINT\n",
    "    LOAD PARQUET \"/tmp/f\" PRINT\n",
    "    LOAD \"/tmp/f.csv\"(header=true) PRINT\n",
    "    LOAD \"/tmp/f.csv\"(header=true) COLUMNS a:int,b:str PRINT\n",
    "    LOAD \"/tmp/f.json\" PRINT\n",
    "    LOAD \"/tmp/f.json\" COLUMNS a:int,b:str PRINT\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PRINT\n",
    "\n",
    "> **PRINT** [dataframes] [**ROWS** int] [**ROWCOUNT**] [**TITLE** \"title\"]\n",
    "\n",
    "When you give `ROWCOUNT` you want to print the row count of the dataframe. In a distributed environment, it can be expensive, so you mmay consider persisting the dataframes you want to print."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def helper(ct=30) -> pd.DataFrame:\n",
    "    np.random.seed(0)\n",
    "    return pd.DataFrame(np.random.randint(0,10,size=(ct, 3)), columns=list('abc'))\n",
    "\n",
    "with FugueSQLWorkflow() as dag:\n",
    "    dag(\"\"\"\n",
    "    CREATE USING helper\n",
    "    PRINT\n",
    "    PRINT ROWS 5 ROWCOUNT TITLE \"xyz\"\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Outputter\n",
    "\n",
    "> **OUTPUT** [dataframes] [**PREPARTITION** statement] **USING** extension [(params)]\n",
    "\n",
    "Only when the extension is implementing `Outputter` interface, it's able to access the prepartition hint.\n",
    "\n",
    "Please also read [this](./outputter.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def output(df:pd.DataFrame, n=1) -> None:\n",
    "    print(n)\n",
    "    print(df)\n",
    "\n",
    "with FugueSQLWorkflow() as dag:\n",
    "    dag(\"\"\"\n",
    "    a=CREATE [[0]] SCHEMA a:int\n",
    "    OUTPUT a USING output(n=2)\n",
    "    OUTPUT PREPARTITION BY a USING output\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save File\n",
    "\n",
    "> **SAVE** [dataframe] [**PREPARTITION** statement] **OVERWRITE|APPEND|TO** [**SINGLE**] [PARQUET|CSV|JSON] path [(params)]\n",
    "\n",
    "`SAVE ... TO` means if the file exists, the error will be thrown.\n",
    "\n",
    "When saving to CSV, normally, you add `(header=true)` to save header."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fugue_spark import SparkExecutionEngine\n",
    "\n",
    "with FugueSQLWorkflow(SparkExecutionEngine) as dag:\n",
    "    dag(\"\"\"\n",
    "    CREATE [[0,\"1\"]] SCHEMA a:int,b:str\n",
    "    SAVE OVERWRITE \"/tmp/f.parquet\"\n",
    "    SAVE OVERWRITE SINGLE PARQUET \"/tmp/f2\"\n",
    "    SAVE PREPARTITION BY a PRESORT b OVERWRITE \"/tmp/f.csv\"(header=true)\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`PREPARTITION` can change the file structure, it can also affect speed for a distributed framework. The following shows how partition changes the output structure using Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def helper(ct=30) -> pd.DataFrame:\n",
    "    np.random.seed(0)\n",
    "    return pd.DataFrame(np.random.randint(0,10,size=(ct, 3)), columns=list('abc'))\n",
    "\n",
    "with FugueSQLWorkflow(SparkExecutionEngine) as dag:\n",
    "    dag(\"\"\"\n",
    "    CREATE USING helper\n",
    "    SAVE OVERWRITE \"/tmp/f3.parquet\"\n",
    "    SAVE PREPARTITION BY a OVERWRITE \"/tmp/f4.parquet\"\n",
    "    \"\"\")\n",
    "    \n",
    "from fugue import FileSystem\n",
    "\n",
    "print(FileSystem().listdir(\"/tmp/f3.parquet\"))\n",
    "print(FileSystem().listdir(\"/tmp/f4.parquet\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Processor\n",
    "\n",
    "> **PROCESS** [dataframes] [**PREPARTITION** statement] **USING** extension [(params)] [**SCHEMA** schema]\n",
    "\n",
    "Only when the extension is implementing `Processor` interface, it's able to access the prepartition hint.\n",
    "\n",
    "Please also read [this](./processor.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat(df1:pd.DataFrame, df2:pd.DataFrame) -> pd.DataFrame:\n",
    "    return pd.concat([df1,df2]).reset_index(drop=True)\n",
    "\n",
    "with FugueSQLWorkflow() as dag:\n",
    "    dag(\"\"\"\n",
    "    a = CREATE [[0,\"1\"]] SCHEMA a:int,b:str\n",
    "    b = CREATE [[1,\"2\"]] SCHEMA a:int,b:str\n",
    "    PROCESS a,b USING concat\n",
    "    PRINT\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Transformer\n",
    "\n",
    "> **TRANSFORM** [dataframe] [**PREPARTITION** statement] **USING** extension [(params)] [**SCHEMA** schema]\n",
    "\n",
    "`PREPARTITION` will control if you want to apply the transformer directly on **physical** partitions or **logical** partitions. For the concept of partition read the [partition tutorial](./partition.ipynb).\n",
    "\n",
    "Please also read [this](./transformer.ipynb)\n",
    "\n",
    "**This is very important section, please read**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Iterable,List,Any\n",
    "from fugue_sql import FugueSQLWorkflow\n",
    "from fugue_spark import SparkExecutionEngine\n",
    "\n",
    "def helper(ct=20) -> pd.DataFrame:\n",
    "    np.random.seed(0)\n",
    "    return pd.DataFrame(np.random.randint(0,10,size=(ct, 3)), columns=list('abc'))\n",
    "\n",
    "# schema: *,partition_count:int\n",
    "def ct(df:List[List[Any]]) -> Iterable[List[Any]]:\n",
    "    c = len(df)\n",
    "    for row in df:\n",
    "        yield row+[c]\n",
    "        \n",
    "dag=FugueSQLWorkflow()\n",
    "dag(\"\"\"\n",
    "    a = CREATE USING helper PERSIST\n",
    "    TRANSFORM a USING ct  # on whatever physical partition a has\n",
    "    PRINT ROWS 100\n",
    "    TRANSFORM a PREPARTITION BY a USING ct  # on user defined logical partition, similar to groupBy-apply\n",
    "    PRINT ROWS 100\n",
    "\"\"\")\n",
    "    \n",
    "dag.run()\n",
    "dag.run(SparkExecutionEngine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see on Spark and native python, the first transform gives different result because they have different ways to partition your data on the physical level. However, they produce the same result when you specify prepartition by a, because this logical partition is defined by you, and across all `ExecutionEngine`, it will be respected.\n",
    "\n",
    "Let's see another example: select top record of each partition, so how we use `PRESORT`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# schema: *\n",
    "def select_top(df:Iterable[List[Any]]) -> List[List[Any]]:\n",
    "    return [next(df)]\n",
    "\n",
    "dag=FugueSQLWorkflow()\n",
    "dag(\"\"\"\n",
    "    a = CREATE USING helper PERSIST\n",
    "    TRANSFORM a USING select_top  # on whatever physical partition a has\n",
    "    PRINT TITLE \"underministic result\"\n",
    "    TRANSFORM a PREPARTITION BY a PRESORT b USING select_top\n",
    "    PRINT TITLE \"smallest of each partition\"\n",
    "    TRANSFORM a PREPARTITION BY a PRESORT b DESC USING select_top\n",
    "    PRINT TITLE \"largest of each partition\"\n",
    "\"\"\")\n",
    "    \n",
    "dag.run()\n",
    "dag.run(SparkExecutionEngine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use CoTransformer\n",
    "\n",
    "> **TRANSFORM** [dataframe] **USING** extension [(params)] [**SCHEMA** schema]\n",
    "\n",
    "The extension must be a `CoTransformer` or compatible functions. And before transform you must use `ZIP`\n",
    "\n",
    "> **ZIP** [dataframes] [joinType] [**BY** cols] [**PRESORT** presort]\n",
    "\n",
    "join types are: `CROSS`, `INNER`, `LEFT OUTER`, `RIGHT OUTER`, `FULL OUTER`. If not specified, the default is `INNER`.\n",
    "\n",
    "Please also read [this](./cotransformer.ipynb)\n",
    "\n",
    "In the following example, inline statement is used for simplicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fugue import DataFrames\n",
    "\n",
    "#schema: res:[str]\n",
    "def to_str_with_key(dfs:DataFrames) -> List[List[Any]]:\n",
    "    return [[[k+\" \"+x.as_array().__repr__() for k,x in dfs.items()]]]\n",
    "\n",
    "with FugueSQLWorkflow() as dag:\n",
    "    dag(\"\"\"\n",
    "    df1 = CREATE [[0,1],[1,3]] SCHEMA a:int,b:int\n",
    "    df2 = CREATE [[0,4],[2,2]] SCHEMA a:int,c:int\n",
    "    df3 = CREATE [[0,2],[1,1],[1,5]] SCHEMA a:int,d:int\n",
    "    \n",
    "    TRANSFORM (ZIP df1,df2,df3) USING to_str_with_key\n",
    "    PRINT\n",
    "    \n",
    "    TRANSFORM (ZIP a=df1,b=df2,c=df3 LEFT OUTER BY a PRESORT b DESC) USING to_str_with_key\n",
    "    PRINT\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SELECT\n",
    "\n",
    "It should be fully compatible with standard `SELECT` syntax. Here are some additions:\n",
    "* `FROM` clause if omitted, it means you want to select from the last dataframe generated by the previous statements\n",
    "* `FROM` clause can have inline Fugue statements such as `CREATE`, `PROCESS`, `TRANSFORM`, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with FugueSQLWorkflow() as dag:\n",
    "    dag(\"\"\"\n",
    "    df1 = CREATE [[0,1],[1,3]] SCHEMA a:int,b:int\n",
    "    \n",
    "    # inline\n",
    "    SELECT df1.*, df2.c FROM df1 INNER JOIN (CREATE [[0,4],[2,2]] SCHEMA a:int,c:int) AS df2\n",
    "        ON df1.a=df2.a\n",
    "    \n",
    "    # no from\n",
    "    SELECT c,b,a\n",
    "    \n",
    "    PRINT\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persist & Broadcast\n",
    "\n",
    "For any statement that outputs a single dataframe, you can persist or broadcast that result.\n",
    "\n",
    "Please also read [this](execution_engine.ipynb#Persist-&-Broadcast)\n",
    "\n",
    "In the following example, there are several reasons to persist df1\n",
    "* df1 is used in both the `SELECT` and `PRINT`, for Spark, without persist, it will run twice\n",
    "* The random number generation has no seed, if it runs for multiple times, it may be inconsistent\n",
    "* It's can be large (we can set ct to large value)\n",
    "\n",
    "\n",
    "We also broad cast df2 because it is really small, broadcast it explicitly is a good idea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Iterable,List,Any\n",
    "from fugue_sql import FugueSQLWorkflow\n",
    "from fugue_spark import SparkExecutionEngine\n",
    "\n",
    "def helper(ct=20) -> pd.DataFrame:\n",
    "    return pd.DataFrame(np.random.randint(0,10,size=(ct, 2)), columns=list('ab'))\n",
    "\n",
    "dag = FugueSQLWorkflow()\n",
    "dag(\"\"\"\n",
    "    df1 = SELECT *, rand() AS rand FROM (CREATE USING helper) PERSIST\n",
    "    df2 = CREATE [[0,2],[2,3]] SCHEMA a:int,c:int BROADCAST\n",
    "    \n",
    "    SELECT df1.*,c FROM df1 INNER JOIN df2 ON df1.a=df2.a\n",
    "    PRINT\n",
    "    PRINT df1\n",
    "    \"\"\")\n",
    "\n",
    "dag.run(SparkExecutionEngine)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
