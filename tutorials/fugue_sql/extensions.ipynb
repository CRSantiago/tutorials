{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# FugueSQL Extensions\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Extensions are Python functions that are wrapped in order to execute in the `FugueSQLWorkflow`. These are needed to implement custom logic in SQL workflows. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Creator\n",
    "\n",
    "`Creators` are functions that generate a `DataFrame`. The example below contains all syntax variations. Schema needs to be specified in the Python code, or in the SQL query. **Pandas** `DataFrames` have schema defined, so it does not need to be passed. The default `LOAD` an example of a `Creator`.\n",
    "\n",
    "A common use case for `Creator` is reading from a different data source like MongoDB Atlas or AWS S3.\n",
    "\n",
    "[Read more about Creators](../creator.ipynb)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fugue_sql import FugueSQLWorkflow\n",
    "from typing import List, Any\n",
    "import pandas as pd\n",
    "\n",
    "def create1(n=1) -> pd.DataFrame:\n",
    "    return pd.DataFrame([[n]],columns=[\"a\"])\n",
    "\n",
    "# schema: a:int\n",
    "def create2(n=1) -> List[List[Any]]:\n",
    "    return [[n]]\n",
    "\n",
    "def create3(n=1) -> List[List[Any]]:\n",
    "    return [[n]]\n",
    "\n",
    "with FugueSQLWorkflow() as dag:\n",
    "    dag(\"\"\"\n",
    "    CREATE [[0,\"hello\"],[1,\"world\"]] SCHEMA a:int,b:str\n",
    "    PRINT\n",
    "    CREATE USING create1 \n",
    "    PRINT\n",
    "    CREATE USING create2(n=3) \n",
    "    PRINT\n",
    "    CREATE USING create3(n=4) SCHEMA a:int PRINT\n",
    "    PRINT\n",
    "    \"\"\")"
   ]
  },
  {
   "source": [
    "## Outputter\n",
    "\n",
    "`Outputters` are functions that either write out `DataFrames` or display them. The default `SAVE` and `PRINT` are examples of Outputters. They do not return anything. They are invoked in SQL using the `OUTPUT` keyword.\n",
    "\n",
    "`PREPARTITION` can be used along with `Outputters` to apply the logic on each partition. This is only possible if the `Outputter interface` is used to define the extension.\n",
    "\n",
    "[Read more about Outputters and the interface](../outputter.ipynb)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output(df:pd.DataFrame, n=1) -> None:\n",
    "    print(n)\n",
    "    print(df)\n",
    "\n",
    "with FugueSQLWorkflow() as dag:\n",
    "    dag(\"\"\"\n",
    "    a=CREATE [[0]] SCHEMA a:int\n",
    "    OUTPUT a USING output(n=2)\n",
    "    OUTPUT PREPARTITION BY a USING output\n",
    "    \"\"\")"
   ]
  },
  {
   "source": [
    "## Processor\n",
    "\n",
    "`Processors` take in multiple `DataFrames` and output one `DataFrame`. Similar to the `Outputter`, the SQL `PROCESS` keyword can be used in conjunction with `PREPARTITION` but only if the `Processor class interface` was used to define the `Processor`.\n",
    "\n",
    "[Read more about Processors](../processor.ipynb)\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat(df1:pd.DataFrame, df2:pd.DataFrame) -> pd.DataFrame:\n",
    "    return pd.concat([df1,df2]).reset_index(drop=True)\n",
    "\n",
    "with FugueSQLWorkflow() as dag:\n",
    "    dag(\"\"\"\n",
    "    a = CREATE [[0,\"1\"]] SCHEMA a:int,b:str\n",
    "    b = CREATE [[1,\"2\"]] SCHEMA a:int,b:str\n",
    "    PROCESS a,b USING concat\n",
    "    PRINT\n",
    "    \"\"\")"
   ]
  },
  {
   "source": [
    "## Transformer\n",
    "\n",
    "`Transformers` are the most used extension. They take one `DataFrame` in and output one `DataFrame`. This has appeared in the previous tutorials. It can be used with `PREPARTITION` to apply the `Transformer` to each parition.\n",
    "\n",
    "[Read more about Transformers](../transformer.ipynb)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    [\"A\", \"2020-01-01\", 10],\n",
    "    [\"A\", \"2020-01-02\", None],\n",
    "    [\"A\", \"2020-01-03\", 30],\n",
    "    [\"B\", \"2020-01-01\", 20],\n",
    "    [\"B\", \"2020-01-02\", None],\n",
    "    [\"B\", \"2020-01-03\", 40]\n",
    "]\n",
    "schema = \"id:str,date:date,value:double\"\n",
    "\n",
    "# schema: *, shift:double\n",
    "def shift(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df['shift'] = df['value'].shift()\n",
    "    return df\n",
    "\n",
    "with FugueSQLWorkflow() as dag:\n",
    "    df = dag.df(data, schema)\n",
    "    dag(\"\"\"\n",
    "    a = SELECT * FROM df\n",
    "    TRANSFORM a PREPARTITION BY id PRESORT date DESC USING shift\n",
    "    PRINT\n",
    "    TRANSFORM a USING shift    # default partition\n",
    "    PRINT\n",
    "    \"\"\")"
   ]
  },
  {
   "source": [
    "**Spark** may give inconsistent results when using `TRANSFORM` without using `PREPARITION` because the default partitions are used. Also note order is not guaranteed in a distributed environment unless explicitly specified. `PREPARTITION` can also be used without a `PRESORT`. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## CoTransformer\n",
    "\n",
    "`CoTransformers` are the used on multiple `DataFrames` parititioned in the same way. The data is then joined together with an `INNER JOIN` by default, but it can be specified which join to use. In `FugueSQL`, `TRANSFORM` and `ZIP` are used together to apply the `CoTransformer`."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fugue import DataFrames\n",
    "\n",
    "#schema: res:[str]\n",
    "def to_str_with_key(dfs:DataFrames) -> List[List[Any]]:\n",
    "    return [[[k+\" \"+x.as_array().__repr__() for k,x in dfs.items()]]]\n",
    "\n",
    "with FugueSQLWorkflow() as dag:\n",
    "    dag(\"\"\"\n",
    "    df1 = CREATE [[0,1],[1,3]] SCHEMA a:int,b:int\n",
    "    df2 = CREATE [[0,4],[2,2]] SCHEMA a:int,c:int\n",
    "    df3 = CREATE [[0,2],[1,1],[1,5]] SCHEMA a:int,d:int\n",
    "\n",
    "    TRANSFORM (ZIP df1,df2,df3) USING to_str_with_key\n",
    "    PRINT\n",
    "\n",
    "    TRANSFORM (ZIP a=df1,b=df2,c=df3 LEFT OUTER BY a PRESORT b DESC) USING to_str_with_key\n",
    "    PRINT\n",
    "    \"\"\")"
   ]
  }
 ]
}