{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fugue-sql\n",
    "\n",
    "`Fugue-sql` is designed for heavy SQL users to extend the boundaries of traditional SQL workflows. `Fugue-sql` allows the expression of logic for end-to-end  distributed computing workflows. It can also be combined with Python code to use custom functions alongside the SQL commands. It provides a unified interface, allowing the same code to run on Pandas, Dask, and Spark\n",
    "\n",
    "The SQL query is parsed with **ANTLR** and mapped to the equivalent functions in the `Fugue` programming interface.\n",
    "\n",
    "## 1. Installation\n",
    "\n",
    "In order to use `FugueSQL`, you first need to make sure you have installed the sql extra\n",
    "```\n",
    "pip install fugue[sql]\n",
    "```\n",
    "To run on Spark or Dask execution engines, install the appropriate extras. Alternatively, `all` can be used as an extra.\n",
    "```\n",
    "pip install fugue[sql, spark] \n",
    "pip install fugue[sql, dask]\n",
    "pip install fugue[all]\n",
    "```\n",
    "\n",
    "## [2. Fugue SQL Syntax](syntax.ipynb)\n",
    "\n",
    "Get started with `fugue-sql`. This shows input and output of data, enhancements over standard SQL, and how to use SQL to describe computation logic. After this, users will be able to use `fugue-sql` with the familiar SQL keywords to perform operations on top of **Pandas**, **Spark**, and **Dask**.\n",
    "\n",
    "## [3. Additional SQL Operators](operators.ipynb)\n",
    "\n",
    "Go over the implemented operations that `Fugue` has on top of the ones provided by standard SQL. `Fugue-sql` is extensible with Python code, but the most common functions are added as built-ins. These include filling NULL values, dropping NULL values, renaming columns, changing schema, etc. This section goes over the most used additional keywords.\n",
    "\n",
    "## [4. Integrating Python](python.ipynb)\n",
    "\n",
    "Explore [Jinja templating](https://jinja.palletsprojects.com/) for variable passing, and using a Python functions as a [Transformer](../transformer.ipynb) in a `%%fsql` cell.\n",
    "\n",
    "## [5. Using Other Fugue Extensions](extensions.ipynb)\n",
    "\n",
    "The [Transformer](../transformer.ipynb) is just one of many possible [Fugue extensions](../extensions.ipynb). In this section we'll explore the syntax of all the other Fugue extensions: [Creator](../creator.ipynb), [Processor](../processor.ipynb), [Outputter](../outputter.ipynb), and [CoTransformer](../cotransformer.ipynb).\n",
    "\n",
    "## 6. Fugue SQL with Pandas\n",
    "\n",
    "`%%fsql` takes in the NativeExecutionEngine as a default parameter. This engine runs on Pandas. All of the SQL operations have equivalents in Pandas, but the behavior can be inconsistent sometimes. For example, Pandas will drop NULL values by default in a groupby operation. The NativeExecutionEngine was designed to mostly make operations consistent with Spark and SQL.\n",
    "\n",
    "## [7. Fugue SQL with Dask](dask.ipynb)\n",
    "\n",
    "`Fugue` and [dask-sql](https://dask-sql.readthedocs.io/en/latest/index.html) are collaborating to have our solutions converge and bring the SQL interface for [Dask](https://docs.dask.org/en/latest/). Currently, `dask-sql` is faster on average, while `fugue-sql` is more complete in terms of `SQL` keywords implemented. Conveniently, our solutions can be used together to bring the best of both worlds. This is done by using `dask-sql` as the underlying [execution engine](../execution_engine.ipynb) of `FugueSQLWorkflow`. \n",
    "\n",
    "## 8. Fugue SQL with Spark\n",
    "\n",
    "`Fugue-sql` also works on **Spark** by passing in the execution engine. This looks like `%%fsql spark`. The operations are mapped to **Spark** and Spark SQL operations. The difference is `fugue-sql` has added functionality for syntax compared to SparkSQL as seen in the [syntax tutorial](syntax.ipynb). Additionally with `fugue-sql`, the same code will execute on Pandas and Dask without modification. This allows for quick testing without having to spin up a cluster. Users prototype with the `NativeExecutionEngine`, and then move to the **Spark** cluster by changing the execution engine."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit ('data_val': conda)",
   "metadata": {
    "interpreter": {
     "hash": "e7204170c0acfd87246dc2d70bda30ce42b59f5d43297c7b5d36656424b405d8"
    }
   },
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}