{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# FugueSQL Operators\n",
    "\n",
    "The previous section talked about `FugueSQL` syntax. Along with the standard SQL operations, `FugueSQL` has implemented some additional keywords (and is adding more). These keywords have equivalent methods in the programming interface. `FugueSQL` aims to make more coding fun and more English-like. Our goal is to provide an intuitive interface that is easy to read.\n",
    "\n",
    "This is not a complete reference, it just contains the most used keywords."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "from fugue_sql import FugueSQLWorkflow\n",
    "\n",
    "# Defining data\n",
    "data = [\n",
    "    [\"A\", \"2020-01-01\", 10],\n",
    "    [\"A\", \"2020-01-02\", None],\n",
    "    [\"A\", \"2020-01-03\", 30],\n",
    "    [\"B\", \"2020-01-01\", 20],\n",
    "    [\"B\", \"2020-01-02\", None],\n",
    "    [\"B\", \"2020-01-03\", 40]\n",
    "]\n",
    "schema = \"id:str,date:date,value:double\""
   ]
  },
  {
   "source": [
    "## Input and Output Operations"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## PRINT\n",
    "\n",
    "Prints a dataframe.\n",
    "\n",
    "Usage:\n",
    "\n",
    "`PRINT [dataframe] [ROWS int] [ROWCOUNT] [TITLE “title”]`\n",
    "\n",
    "* dataframe - If not provided, takes the last dataframe.\n",
    "* ROWS - Number of rows.\n",
    "* ROWCOUNT - Displays number of rows for dataframe. This is expensive for Spark and Dask. For distributed environments, persisting will help before doing this operation.\n",
    "* TITLE - Title for display"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example for PRINT\n",
    "with FugueSQLWorkflow() as dag:\n",
    "    dag(\"\"\"\n",
    "    df = CREATE [[0,\"hello\"],[1,\"world\"]] SCHEMA a:int,b:str\n",
    "    PRINT df ROWS 2 ROWCOUNT TITLE \"xyz\" \n",
    "    \"\"\")"
   ]
  },
  {
   "source": [
    "## LOAD\n",
    "\n",
    "Loads a CSV, JSON, or PARQUET file as a `DataFrame`.\n",
    "\n",
    "Usage:\n",
    "\n",
    "`LOAD [PARQUET|CSV|JSON] \"path\" (params) [COLUMNS schema|columns]`\n",
    "\n",
    "* PARQUET|CSV|JSON - File type to load. Required if the file has no extension.\n",
    "* path - File path to load.\n",
    "* params - Passed on to underlying execution engine loading method.\n",
    "* COLUMNS - Columns to grab or schema to load it in as."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## SAVE (or SAVE AND USE)\n",
    "\n",
    "Saves a CSV, JSON, or PARQUET file as a `DataFrame`. `SAVE AND USE` just returns the dataframe so there is no need to load it back in.\n",
    "\n",
    "Usage:\n",
    "\n",
    "`SAVE [dataframe] [PREPARTITION statement] [OVERWRITE|APPEND|TO] [SINGLE] [PARQUET|CSV|JSON] \"path\" [(params)]`\n",
    "\n",
    "or \n",
    "\n",
    "`SAVE AND USE [dataframe] [PREPARTITION statement] [OVERWRITE|APPEND|TO] [SINGLE] [PARQUET|CSV|JSON] \"path\" [(params)]`\n",
    "\n",
    "* dataframe - If not provided, takes the last dataframe.\n",
    "* PREPARTITION - Partitions for file.\n",
    "* OVERWRITE|APPEND|TO - Choose the mode for writing the file out. `TO` throws an error if the file exists.\n",
    "* SINGLE - One file output.\n",
    "* PARQUET|CSV|JSON - Choose file type (Parquet, CSV, or JSON) for output. Required if path has no extension.\n",
    "* path - File path to write out to.\n",
    "* params - Passed on to underlying execution engine saving method."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE and LOAD example\n",
    "with FugueSQLWorkflow() as dag:\n",
    "    dag(\"\"\"\n",
    "    CREATE [[0,\"1\"]] SCHEMA a:int,b:str\n",
    "    SAVE OVERWRITE \"/tmp/f.parquet\"\n",
    "    SAVE OVERWRITE \"/tmp/f.csv\" (header=true)\n",
    "    SAVE OVERWRITE \"/tmp/f.json\"\n",
    "    SAVE OVERWRITE PARQUET \"/tmp/f\"\n",
    "    \"\"\")\n",
    "    dag(\"\"\"\n",
    "    LOAD \"/tmp/f.parquet\" PRINT\n",
    "    LOAD \"/tmp/f.parquet\" COLUMNS a PRINT\n",
    "    LOAD PARQUET \"/tmp/f\" PRINT\n",
    "    LOAD \"/tmp/f.csv\" (header=true) PRINT\n",
    "    LOAD \"/tmp/f.csv\" (header=true) COLUMNS a:int,b:str PRINT\n",
    "    LOAD \"/tmp/f.json\" PRINT\n",
    "    LOAD \"/tmp/f.json\" COLUMNS a:int,b:str PRINT\n",
    "    \"\"\")"
   ]
  },
  {
   "source": [
    "## Partitioning\n",
    "\n",
    "Partitioning is an important part of distributed computing. We arrange the data into different logical partitions and then perform operations. This is normally used in conjunction with Fugue extensions. This is a clause that as part of statements."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## PREPARTITION\n",
    "\n",
    "Partitions a dataframe in preparation for a following operation.\n",
    "\n",
    "Usage:\n",
    "\n",
    "`PREPARTITION [RAND|HASH|EVEN] [number] [BY columns] [PRESORT statement]`\n",
    "\n",
    "\n",
    "* RAND|HASH|EVEN - Algorithm for prepartition. Read [this](../partition.ipynb).\n",
    "* number - Number of partitions.\n",
    "* columns - What columns to partition on.\n",
    "* statement - Presort hint. Check `PRESORT` syntax."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## PRESORT\n",
    "\n",
    "Usage:\n",
    "\n",
    "`PRESORT column [ASC|DESC]`\n",
    "\n",
    "Defines a presort before another operation. This is a clause mainly used with `PREPARTITION`. Multiple column, order pairs can be used separated by `,`.\n",
    "\n",
    "* column - Name of columns to sort on.\n",
    "* ASC|DESC - Order of sort."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "The example below shows how to use `PREPARTITION` and `PRESORT`. We need to define a transformer to apply it with."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPARTITION and PRESORT example\n",
    "import pandas as pd\n",
    "\n",
    "# schema: *, shift:double\n",
    "def shift(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df['shift'] = df['value'].shift()\n",
    "    return df\n",
    "\n",
    "with FugueSQLWorkflow() as dag:\n",
    "    df = dag.df(data, schema)    # data and schema defined at top\n",
    "    dag(\"\"\"\n",
    "    TRANSFORM df PREPARTITION BY id PRESORT date ASC USING shift\n",
    "    PRINT\n",
    "    \"\"\")"
   ]
  },
  {
   "source": [
    "## Column and Schema Opeartions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## RENAME COLUMNS\n",
    "\n",
    "Usage:\n",
    "\n",
    "`RENAME COLUMNS params [FROM dataframe]`\n",
    "\n",
    "* params : Pairs of old_name:new_name separated by `,`.\n",
    "* dataframe: If none is provided, take the previous one."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## ALTER COLUMNS\n",
    "\n",
    "Changes data type of columns.\n",
    "\n",
    "Usage:\n",
    "\n",
    "`ALTER COLUMNS params [FROM dataframe]`\n",
    "\n",
    "* params : Pairs of column:dtype separated by `,`.\n",
    "* dataframe - If not provided, takes the last one."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## DROP COLUMNS\n",
    "\n",
    "Drops columns from `DataFrame`.\n",
    "\n",
    "Usage:\n",
    "\n",
    "`DROP COLUMNS colnames [IF EXISTS] [FROM dataframe]`\n",
    "\n",
    "* colnames - Column names separated by `'`.\n",
    "* IF EXISTS - Drops if the column exists, otherwise error.\n",
    "* dataframe - If not provided, takes the last."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example for RENAME COLUMNS, ALTER COLUMNS, DROP COLUMNS\n",
    "with FugueSQLWorkflow() as dag:\n",
    "    dag(\"\"\"\n",
    "    df = CREATE [[0,\"1\"]] SCHEMA a:int,b:str\n",
    "    df2 = RENAME COLUMNS a:aa b:bb FROM df\n",
    "    PRINT df2\n",
    "    df3 = ALTER COLUMNS aa:str, bb:int FROM df2\n",
    "    PRINT df3\n",
    "    df4 = DROP COLUMNS bb, c IF EXISTS FROM df3\n",
    "    PRINT df4\n",
    "    \"\"\"\n",
    "    )"
   ]
  },
  {
   "source": [
    "## NULL Handling"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## DROP ROWS\n",
    "\n",
    "Drops rows from `DataFrame` containing NULLs.\n",
    "\n",
    "Usage:\n",
    "\n",
    "`DROP ROWS IF ALL|ANY NULL|NULLS [ON columns] [FROM dataframe]`\n",
    "\n",
    "* ALL|ANY - All values are NULL or any value is NULL in the row of data.\n",
    "* NULL|NULLS - There is no difference.\n",
    "* columns - Columns to check for NULL values.\n",
    "* dataframe - If not provided, takes the last."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## FILL\n",
    "\n",
    "Fills values from `DataFrame` containing NULLs.\n",
    "\n",
    "Usage:\n",
    "\n",
    "`FILL NULL|NULLS PARAMS params [FROM dataframe]`\n",
    "\n",
    "* NULL|NULLS - There is no difference\n",
    "* params - Pairs of column_name:fill_value\n",
    "* dataframe - If not provided, takes the last dataframe"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example for DROP ROWS and FILL\n",
    "with FugueSQLWorkflow() as dag:\n",
    "    dag(\"\"\"\n",
    "    df = CREATE [[NULL,\"1\"]] SCHEMA a:double,b:str\n",
    "    df2 = DROP ROWS IF ANY NULL ON a FROM df\n",
    "    PRINT df2\n",
    "    df3 = DROP ROWS IF ALL NULLS FROM df\n",
    "    PRINT df3\n",
    "    df4 = FILL NULLS PARAMS a:1 FROM df\n",
    "    PRINT df4\n",
    "    \"\"\"\n",
    "    )"
   ]
  },
  {
   "source": [
    "## Sampling"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## SAMPLE\n",
    "\n",
    "Takes a sample of the `DataFrame`, potentially with replacement. Use either number of rows or percent of dataframe.\n",
    "\n",
    "Usage:\n",
    "\n",
    "`SAMPLE [REPLACE] [rows ROWS | percent PERCENT] [SEED seed] [FROM dataframe]`\n",
    "\n",
    "* REPLACE - Sample with replacement\n",
    "* rows - Integer for number of rows.\n",
    "* percent - Integer or Decimal indicating percent of dataframe to be returned\n",
    "* seed - Random seed for sampling\n",
    "* dataframe - If not provided, takes the last dataframe"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## TAKE\n",
    "\n",
    "TAKE is equivalent to `Pandas head`. It returns the top rows of a `DataFrame`. If used with `PREPARTITION`, it returns the top rows of each partition. `PRESPORT` can be applied before taking the top rows.\n",
    "\n",
    "Usage:\n",
    "\n",
    "`TAKE rows ROW|ROWS [FROM dataframe ][PREPARTITION statement] [NULL|NULLS FIRST|LAST]`\n",
    "\n",
    "* rows - Integer for number of rows.\n",
    "* dataframe - If not provided, takes the last dataframe.\n",
    "* PREPARTITION - See syntax for `PREPARTITION`.\n",
    "* NULL|NULLS - No difference.\n",
    "* FIRST|LAST - If there is a `PRESORT`, sort with NULLS at the top or NULLS at the bottom."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example for SAMPLE and TAKE\n",
    "with FugueSQLWorkflow() as dag:\n",
    "    dag(\"\"\"\n",
    "    df = CREATE [[1,\"1\"],[2,\"2\"],[3,\"3\"],[4,\"4\"],[5,\"5\"]] SCHEMA a:double,b:str\n",
    "    df2 = SAMPLE 2 ROWS SEED 42 FROM df\n",
    "    PRINT df2\n",
    "    df3 = SAMPLE 40 PERCENT SEED 42 FROM df\n",
    "    PRINT df3\n",
    "    df4 = TAKE 3 ROWS FROM df\n",
    "    PRINT df4\n",
    "    df5 = TAKE 1 ROW FROM df PREPARTITION BY a   # Returns 1 row for each partition\n",
    "    PRINT df5\n",
    "    \"\"\"\n",
    "    )"
   ]
  },
  {
   "source": [
    "## Distributed Computing Operations\n",
    "\n",
    "These next keywords are used for distributed environments to save repeated computation. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## BROADCAST\n",
    "\n",
    "Copies a DataFrame (ideally a small one) to worker nodes to prevent shuffling when joining to larger dataframes. This is used after any `FugueSQL` statement that outputs a `DataFrame`. It is used by adding it to the end of a statement.\n",
    "\n",
    "Sample Usage:\n",
    "\n",
    "`TAKE 2 ROWS FROM df BROADCAST`"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## PERSIST or CHECKPOINT\n",
    "\n",
    "Caches a dataframe. Fugue has many types of `CHECKPOINT`. Please read [this](../checkpoint.ipynb) for a deep dive when to use each type. Similar to `BROADCAST`, it it used by appending the keyword after another `FugueSQL` statement that outputs a `DataFrame`.\n",
    "\n",
    "Sample Usage:\n",
    "\n",
    "`TAKE 2 ROWS FROM df PERSIST`\n",
    "\n",
    "`TAKE 2 ROWS FROM df WEAK CHECKPOINT`"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}