{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# FugueSQL Operators\n",
    "\n",
    "The previous section talked about FugueSQL syntax. Along with the standard SQL operations, FugueSQL has implemented some additional keywords (and is adding more). These keywords have equivalent methods in the programming interface. Some of them can be executed in standard SQL, but others would be a bit trickier.\n",
    "\n",
    "FugueSQL aims to make coding fun and more English-like. Instead of outlining the rules, our goal is to provide an intuitive interface."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "from fugue_sql import FugueSQLWorkflow\n",
    "\n",
    "# Defining data\n",
    "data = [\n",
    "    [\"A\", \"2020-01-01\", 10],\n",
    "    [\"A\", \"2020-01-02\", None],\n",
    "    [\"A\", \"2020-01-03\", 30],\n",
    "    [\"B\", \"2020-01-01\", 20],\n",
    "    [\"B\", \"2020-01-02\", None],\n",
    "    [\"B\", \"2020-01-03\", 40]\n",
    "]\n",
    "schema = \"id:str,date:date,value:double\""
   ]
  },
  {
   "source": [
    "## Input and Output Operations"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## PRINT\n",
    "\n",
    "Loads a CSV, JSON, or PARQUET file as a DataFrame\n",
    "* DATAFRAME - If not provided, takes the last\n",
    "* ROWS - Number of rows\n",
    "* ROWCOUNT - Displays number of rows for dataframe. This is expensive for Spark and Dask. For distributed environments, persisting might help before doing thi soperation.\n",
    "* TITLE - Title for display. \n",
    "\n",
    "Usage:\n",
    "\n",
    "`PRINT [dataframes] [ROWS int] [ROWCOUNT] [TITLE “title”]`"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with FugueSQLWorkflow() as dag:\n",
    "    dag(\"\"\"\n",
    "    CREATE [[0,\"hello\"],[1,\"world\"]] SCHEMA a:int,b:str\n",
    "    PRINT ROWS 2 ROWCOUNT TITLE \"xyz\" \n",
    "    \"\"\")"
   ]
  },
  {
   "source": [
    "## LOAD\n",
    "\n",
    "Loads a CSV, JSON, or PARQUET file as a DataFrame\n",
    "* FILE TYPE - Parquet, CSV, JSON\n",
    "* PARAMS - Passed on to underlying execution engine loading method\n",
    "* COLUMNS - Columns to grab or schema to load it in as\n",
    "\n",
    "Usage:\n",
    "\n",
    "`LOAD [PARQUET|CSV|JSON] path (params) [COLUMNS schema|columns]`"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## SAVE (or SAVE AND USE)\n",
    "\n",
    "Saves a CSV, JSON, or PARQUET file as a DataFrame. SAVE AND USE just returns the dataframe so there is no need to load it back in.\n",
    "\n",
    "* DATAFRAME - If not provided, takes the last\n",
    "* PREPARTITION - Partitions for file\n",
    "* MODE - Overwrite, append, or to (error if exists)\n",
    "* SINGLE - One file output\n",
    "* FILE TYPE - Parquet, CSV, JSON\n",
    "* PARAMS - Passed on to underlying execution engine loading method\n",
    "\n",
    "Usage:\n",
    "\n",
    "`SAVE [dataframe] [PREPARTITION statement] [OVERWRITE|APPEND|TO] [SINGLE] [PARQUET|CSV|JSON] path [(params)]`\n",
    "\n",
    "or \n",
    "\n",
    "`SAVE AND USE [dataframe] [PREPARTITION statement] [OVERWRITE|APPEND|TO] [SINGLE] [PARQUET|CSV|JSON] path [(params)]`"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and Load example\n",
    "with FugueSQLWorkflow() as dag:\n",
    "    dag(\"\"\"\n",
    "    CREATE [[0,\"1\"]] SCHEMA a:int,b:str\n",
    "    SAVE OVERWRITE \"/tmp/f.parquet\"\n",
    "    SAVE OVERWRITE \"/tmp/f.csv\" (header=true)\n",
    "    SAVE OVERWRITE \"/tmp/f.json\"\n",
    "    SAVE OVERWRITE PARQUET \"/tmp/f\"\n",
    "    \"\"\")\n",
    "    dag(\"\"\"\n",
    "    LOAD \"/tmp/f.parquet\" PRINT\n",
    "    LOAD \"/tmp/f.parquet\" COLUMNS a PRINT\n",
    "    LOAD PARQUET \"/tmp/f\" PRINT\n",
    "    LOAD \"/tmp/f.csv\" (header=true) PRINT\n",
    "    LOAD \"/tmp/f.csv\" (header=true) COLUMNS a:int,b:str PRINT\n",
    "    LOAD \"/tmp/f.json\" PRINT\n",
    "    LOAD \"/tmp/f.json\" COLUMNS a:int,b:str PRINT\n",
    "    \"\"\")"
   ]
  },
  {
   "source": [
    "## Partitioning\n",
    "\n",
    "Partitioning is an important part of distributed computing. We arrange the data into different logical partitions and then perform operations. This is normally used in conjunction with Fugue extensions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## PREPARTITION\n",
    "\n",
    "Partitions a dataframe in preparation for a following operation. Use either NUMBER or BY.\n",
    "* ALGO - RAND, HASH, or EVEN\n",
    "* NUMBER - Number of partition\n",
    "* BY - What columns to partition on\n",
    "* PREPARTITION - Partitions for file\n",
    "* PRESORT - Presort hint. Check PRESORT syntax\n",
    "\n",
    "Usage:\n",
    "\n",
    "`TRANSFORM df [RAND|HASH|EVEN] PREPARTITION BY a,b PRESORT c DESC USING count`\n",
    "\n",
    "or\n",
    "\n",
    "`TRANSFORM df [RAND|HASH|EVEN] PREPARTITION 100 USING count`"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## PRESORT\n",
    "\n",
    "Defines a presort before another operation. This is never used alone.\n",
    "\n",
    "* COLUMN - Column in DataFrame\n",
    "* ORDER - ASC or DESC\n",
    "\n",
    "Usage:\n",
    "\n",
    "`PRESORT a DESC, b ASC`"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "The example below shows how to use PREPARTITION and PRESORT. We need to define a transformer to apply it with."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partitioning Example. We need to make a Fugue Transformer first\n",
    "import pandas as pd\n",
    "\n",
    "# schema: *, shift:double\n",
    "def shift(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df['shift'] = df['value'].shift()\n",
    "    return df\n",
    "\n",
    "with FugueSQLWorkflow() as dag:\n",
    "    df = dag.df(data, schema)    # data and schema defined at top\n",
    "    dag(\"\"\"\n",
    "    TRANSFORM df PREPARTITION BY id PRESORT date ASC USING shift\n",
    "    PRINT\n",
    "    \"\"\")"
   ]
  },
  {
   "source": [
    "## Column and Schema Opeartions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## RENAME COLUMNS\n",
    "\n",
    "* PARAMS: Pairs of old_name:new:name\n",
    "* DATAFRAME - If not provided, takes the last\n",
    "\n",
    "Usage:\n",
    "\n",
    "`RENAME COLUMNS a:aa, b:bb` FROM df"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## ALTER COLUMNS\n",
    "\n",
    "Changes data type of columns\n",
    "\n",
    "* PARAMS - Pairs of column_name:dtype\n",
    "* DATAFRAME - If not provided, takes the last\n",
    "\n",
    "Usage:\n",
    "\n",
    "`ALTER COLUMNS a:int, b:int from df`"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## DROP COLUMNS\n",
    "\n",
    "Drops columns from DataFrame\n",
    "\n",
    "* COLUMNS - Column names\n",
    "* IF EXISTS - Drops if the column exists. Otherwise error.\n",
    "* DATAFRAME - If not provided, takes the last\n",
    "\n",
    "Usage:\n",
    "\n",
    "`DROP COLUMNS a, b IF EXISTS FROM df`"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## NULL Handling"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## DROP ROWS\n",
    "\n",
    "Drops rows from DataFrame containing NULLs\n",
    "\n",
    "* HOW - ALL or ANY (all NULL or any value is NULL)\n",
    "* NULL - NULL or NULLS. There is no difference.\n",
    "* ON - Columns to include for operation\n",
    "* FROM - If not provided, takes the last\n",
    "\n",
    "\n",
    "Usage:\n",
    "\n",
    "`DROP ROWS IF ANY NULLS ON a FROM df`\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## FILL\n",
    "\n",
    "Fills values from DataFrame containing NULLs\n",
    "\n",
    "* NULL - NULL or NULLS. There is no difference.\n",
    "\n",
    "\n",
    "Usage:\n",
    "\n",
    "`FILL NULLS PARAMS a:99, b:-99 FROM df`\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Sampling"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE REPLACE? method=fugueSampleMethod (SEED seed=INTEGER_VALUE)? (FROM df=fugueDataFrame)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TAKE (rows=INTEGER_VALUE (ROW|ROWS))? (FROM df=fugueDataFrame)? ((partition=fuguePrepartition)|(PRESORT presort=fugueColsSort))? ((NULL|NULLS) na_position=(FIRST|LAST))?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ZIP dfs=fugueDataFrames (how=fugueZipType)? (BY by=fugueCols)? (PRESORT presort=fugueColsSort)?\n",
    "    ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    : LAZY? (PERSIST | WEAK CHECKPOINT) (params=fugueParams)?                                                                                                   #fugueCheckpointWeak\n",
    "    | LAZY? STRONG? CHECKPOINT (partition=fuguePrepartition)? (single=fugueSingleFile)? (params=fugueParams)?                                                   #fugueCheckpointStrong\n",
    "    | LAZY? DETERMINISTIC CHECKPOINT (ns=fugueCheckpointNamespace)? (partition=fuguePrepartition)? (single=fugueSingleFile)? (params=fugueParams)? fugueYield?  #fugueCheckpointDeterministic\n",
    "    | fugueYield    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ": YIELD (AS name=fugueIdentifier)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BROADCAST"
   ]
  }
 ]
}