{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# FugueSQL Syntax\n",
    "\n",
    "\n",
    "The `FugueSQL` syntax is between standard SQL, JSON, and Python. The goals are\n",
    "* To be fully compatible with standard `SQL SELECT` statement\n",
    "* To minimize syntax overhead, to make code as short as possible while still easy to read\n",
    "* Allow users to fully describe their compute logic in SQL as opposed to Python\n",
    "\n",
    "To achieve these goals, enhancements were made to the standard SQL syntax that will be demonstrated here."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Hello World\n",
    "\n",
    "First, we start with the basic syntax `FugueSQL`. We import `FugueSQLWorkflow`, which is based off of `FugueWorkflow`. Fugue has both a programming interface, and SQL interface which have equivalent functionality. `Fugue` constructs a `Directed Acyclic Graph (DAG)` and evaluates it lazily. The `DAG` allows for compile-time validation, to ensure that incorrect code fails quickly. This is more important when Fugue is used with **Spark** and **Dask**. Catching failures quickly on these large computation environments saves users from expensive failed program runs. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fugue_sql import FugueSQLWorkflow\n",
    "\n",
    "with FugueSQLWorkflow() as dag:\n",
    "    dag(\"\"\"\n",
    "    CREATE [[0,\"hello\"],[1,\"world\"]] SCHEMA number:int,word:str\n",
    "    PRINT\n",
    "    \"\"\")"
   ]
  },
  {
   "source": [
    "The `CREATE` keyword here is a Fugue keyword. We'll dive into [extensions](..extensions.ipynb) later and learn more about integrating Python functions into FugueSQL."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## SQL Compliant\n",
    "\n",
    "All standard SQL keywords are available in Fugue SQL. In this example, `GROUP BY`, `WHERE`, `SELECT`, `FROM` are all the same as standard SQL."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining data\n",
    "data = [\n",
    "    [\"A\", \"2020-01-01\", 10],\n",
    "    [\"A\", \"2020-01-02\", None],\n",
    "    [\"A\", \"2020-01-03\", 30],\n",
    "    [\"B\", \"2020-01-01\", 20],\n",
    "    [\"B\", \"2020-01-02\", None],\n",
    "    [\"B\", \"2020-01-03\", 40]\n",
    "]\n",
    "schema = \"id:str,date:date,value:double\"\n",
    "\n",
    "with FugueSQLWorkflow() as dag:\n",
    "    df = dag.df(data, schema)\n",
    "    dag(\"\"\"\n",
    "    SELECT id, date, MIN(value) value\n",
    "    FROM df\n",
    "    WHERE value > 20\n",
    "    GROUP BY id\n",
    "    PRINT\n",
    "    \"\"\")"
   ]
  },
  {
   "source": [
    "Note that the DataFrame `df` was accessed inside the SQL expression. Fugue DataFrames defined are automatically accessible by the DAG. Other variables need to be passed in through [Jinja templating](../syntax.ipynb). More on this will be shown when we explore how Python and FugueSQL interact."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Input and Output\n",
    "\n",
    "Instead of creating the `DataFrame` inside the `FugueSQLWorkflow`, actual data work often require loading in the `DataFrame`. `Fugue` has two keywords in `SAVE` and `LOAD`. Using these allow SQL users to orchestrate their ETL jobs with SQL logic. A csv file can be loaded in, transformed, and then saved elsewhere.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with FugueSQLWorkflow() as dag:\n",
    "    dag(\"\"\"\n",
    "    CREATE [[0,\"1\"]] SCHEMA a:int,b:str\n",
    "    SAVE OVERWRITE \"/tmp/f.parquet\"\n",
    "    SAVE OVERWRITE \"/tmp/f.csv\" (header=true)\n",
    "    SAVE OVERWRITE \"/tmp/f.json\"\n",
    "    SAVE OVERWRITE PARQUET \"/tmp/f\"\n",
    "    \"\"\")\n",
    "    dag(\"\"\"\n",
    "    LOAD \"/tmp/f.parquet\" PRINT\n",
    "    LOAD \"/tmp/f.parquet\" COLUMNS a PRINT\n",
    "    LOAD PARQUET \"/tmp/f\" PRINT\n",
    "    LOAD \"/tmp/f.csv\" (header=true) PRINT\n",
    "    LOAD \"/tmp/f.csv\" (header=true) COLUMNS a:int,b:str PRINT\n",
    "    LOAD \"/tmp/f.json\" PRINT\n",
    "    LOAD \"/tmp/f.json\" COLUMNS a:int,b:str PRINT\n",
    "    \"\"\")"
   ]
  },
  {
   "source": [
    "json, csv, and parquet are support file formats. There are plans to support avro. Notice that parameters can be passed. If running on the default [execution engine](../execution_engine.ipynb), these would be passed on to **Pandas** `read_csv` and `to_csv`.  The file extension is used as a hint to use the appropriate load/save function. If the extension is not present in the filename, it has to be specified."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Variable Assignment\n",
    "\n",
    "From here, it should be getting clear that `Fugue` extends SQL in order to make it a more complete language. One of the additional features is variable assignment. Along with this, multiple `SELECT` statements can be used. This is the equivalent of temp tables or Common Table Expressions (CTE) in SQL."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with FugueSQLWorkflow() as dag:\n",
    "    df = dag.df([[0,\"hello\"],[1,\"world\"]],\"number:int,word:str\")\n",
    "    dag(\"\"\"\n",
    "    SELECT * FROM df\n",
    "    SAVE OVERWRITE \"/tmp/f.csv\"(header=true)\n",
    "\n",
    "    a = LOAD \"/tmp/f.csv\" (header=true)\n",
    "    temp = SELECT * FROM a WHERE number=1\n",
    "    output = SELECT word FROM temp\n",
    "    SAVE OVERWRITE \"/tmp/output.csv\"(header=true)\n",
    "\n",
    "    new_a = LOAD \"/tmp/output.csv\"(header=true)\n",
    "    PRINT new_a\n",
    "    \"\"\")"
   ]
  },
  {
   "source": [
    "## Execution Engine\n",
    "\n",
    "So far, we've only dealt with the default [execution engine](../execution_engine.ipynb). If nothing is passed to the `FugueSQLWorkflow`, the `NativeExecutionEngine` is used. Similar to `Fugue` programming interface, the `execution engine` can easily be changed by passing it to `FugueSQLWorkflow`. Below is an example for Spark.\n",
    "\n",
    "Take note of the output `DataFrame` in the example below. It will be a `SparkDataFrame`."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fugue_spark import SparkExecutionEngine\n",
    "with FugueSQLWorkflow(SparkExecutionEngine) as dag:\n",
    "    df = dag.df([[0,\"hello\"],[1,\"world\"]],\"a:int,b:str\")\n",
    "    dag(\"\"\"\n",
    "    SELECT * FROM df WHERE a=0  # see we can use df directly defined outside\n",
    "    PRINT\n",
    "    \"\"\")"
   ]
  },
  {
   "source": [
    "## Anonymity\n",
    "\n",
    "In `FugueSQL`, one of the simplifications is anonymity. Itâ€™s optional, but it usually can significantly simplify your code and make it more readable.\n",
    "\n",
    "For a statement that only needs to consume the previous dataframe, a `FROM` keyword is not needed. `PRINT` is the best example. `SAVE` is another example. This is can be applied to other keywords. In this example we'll use the `TAKE` function that just returns the number of rows specified."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with FugueSQLWorkflow() as dag:\n",
    "    df = dag.df([[0,\"hello\"],[1,\"world\"]],\"number:int,word:str\")\n",
    "    dag(\"\"\"\n",
    "    a = SELECT * FROM df\n",
    "    TAKE 2 ROWS PRESORT number DESC       # a is consumed by TAKE\n",
    "\n",
    "    b = SELECT * FROM df\n",
    "    TAKE 2 ROWS FROM b PRESORT number DESC\n",
    "    PRINT\n",
    "    \"\"\")"
   ]
  },
  {
   "source": [
    "## Inline Statements\n",
    "\n",
    "The last enchancement is inline statements. One statement can be written in another in between `(` `)` . Anonymity and variable assignment often make this unneeded, but it's just good to know that this option exists."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with FugueSQLWorkflow() as dag:\n",
    "    dag(\"\"\"\n",
    "    a = CREATE [[0,\"hello\"], [1,\"world\"]] SCHEMA number:int,word:str\n",
    "    SELECT *\n",
    "    FROM (TAKE 1 ROW FROM a)\n",
    "    PRINT\n",
    "    \"\"\")"
   ]
  },
  {
   "source": [
    "## Multiple `dag` Calls\n",
    "\n",
    "DataFrames in precending `dag` calls can be used in future `dag` calls. One use case of this is chaining Python programming along with `FugueSQL`."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with FugueSQLWorkflow() as dag:\n",
    "    dag(\"\"\"\n",
    "    a=CREATE [[0,\"hello\"],[1,\"world\"]] SCHEMA number:int,word:str\n",
    "    \"\"\")\n",
    "    # This other_df will be joined in following dag\n",
    "    other_df = dag.df([[0,\"hello2\"],[1,\"world2\"]],\"number:int,word2:str\")\n",
    "    dag(\"\"\"\n",
    "    SELECT a.number num, other_df.word2 \n",
    "    FROM a \n",
    "    INNER JOIN other_df\n",
    "    ON a.number = other_df.number\n",
    "    PRINT\n",
    "    \"\"\")"
   ]
  },
  {
   "source": [
    "This example above shows the possibility of combining Python and SQL workflows. This is useful if Python needs to connect to other place (AWS S3, Azure Blob Storage, Google Analytics) to retrieve data that is needed for the compute workflow. The data can be loaded in with Python and passed to `FugueSQLWorkflow`. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "In this tutorial we have gone through how to use standard SQL operations (and more) on top of Pandas, Spark, and Dask. We have also seen enhancements over standard SQL like anonymity and variable assignment.\n",
    "\n",
    "In a [following section](python.ipynb) we'll look at more ways of integrating Python with `FugueSQLWorkflow` to extend the capabilities of using SQL."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}