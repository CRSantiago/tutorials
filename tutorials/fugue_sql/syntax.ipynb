{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fugue-sql Syntax\n",
    "\n",
    "\n",
    "The `fugue-sql` syntax is between standard SQL, JSON, and Python. The goals are\n",
    "* To be fully compatible with standard `SQL SELECT` statement\n",
    "* To minimize syntax overhead, to make code as short as possible while still easy to read\n",
    "* Allow users to fully describe their compute logic in SQL as opposed to Python\n",
    "\n",
    "To achieve these goals, enhancements were made to the standard SQL syntax that will be demonstrated here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hello World\n",
    "\n",
    "First, we start with the basic syntax `fugue-sql`. We import `fugue_notebook`, which contains a Jupyter notebook extension. Fugue has both a Python interface, and SQL interface which have equivalent functionality. \n",
    "\n",
    "The `setup` function in the cell below provides syntax highlighting for `fugue-sql` users. At the moment, syntax highlighting will not work for JupyterLab notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fugue_notebook import setup\n",
    "setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%fsql\n",
    "CREATE [[0,\"hello\"],[1,\"world\"]] SCHEMA number:int,word:str\n",
    "PRINT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `CREATE` keyword here is a `fugue-sql` keyword. We'll dive into [extensions](..extensions.ipynb) later and learn more about integrating Python functions into fugue-sql."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL Compliant\n",
    "\n",
    "All standard SQL keywords are available in `fugue-sql`. In this example, `GROUP BY`, `WHERE`, `SELECT`, `FROM` are all the same as standard SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining data\n",
    "import pandas as pd\n",
    "data = pd.DataFrame({\"id\": [\"A\",\"A\",\"A\",\"B\",\"B\",\"B\"],\n",
    "                    \"date\": [\"2020-01-01\", \"2020-01-02\",\n",
    "                             \"2020-01-03\", \"2020-01-01\", \n",
    "                             \"2020-01-02\", \"2020-01-03\"],\n",
    "                    \"value\": [10, None, 30, 20, None, 40]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%fsql\n",
    "SELECT id, date, MIN(value) value\n",
    "FROM data\n",
    "WHERE value > 20\n",
    "GROUP BY id\n",
    "PRINT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the Pandas DataFrame `df` was accessed inside the SQL expression. DataFrames defined in Python cells are automatically accessible by SQL cells. Other variables need to be passed in through [Jinja templating](../syntax.ipynb). More on this will be shown when we explore how Python and fugue-sql interact.\n",
    "\n",
    "This example above shows the possibility of combining Python and SQL workflows. This is useful if Python needs to connect to other place (AWS S3, Azure Blob Storage, Google Analytics) to retrieve data that is needed for the compute workflow. The data can be loaded in with Python and passed to `%%fsql` cells. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input and Output\n",
    "\n",
    "Actual data work often require loading in the `DataFrame`. `Fugue` has two keywords in `SAVE` and `LOAD`. Using these allow `fugue-sql` users to orchestrate their ETL jobs with SQL logic. A csv file can be loaded in, transformed, and then saved elsewhere. Full data analysis and transformation workflows can be done in `fugue-sql`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%fsql\n",
    "CREATE [[0,\"1\"]] SCHEMA a:int,b:str\n",
    "SAVE OVERWRITE \"/tmp/f.parquet\"\n",
    "SAVE OVERWRITE \"/tmp/f.csv\" (header=true)\n",
    "SAVE OVERWRITE \"/tmp/f.json\"\n",
    "SAVE OVERWRITE PARQUET \"/tmp/f\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%fsql\n",
    "LOAD \"/tmp/f.parquet\" PRINT\n",
    "LOAD \"/tmp/f.parquet\" COLUMNS a PRINT\n",
    "LOAD PARQUET \"/tmp/f\" PRINT\n",
    "LOAD \"/tmp/f.csv\" (header=true) PRINT\n",
    "LOAD \"/tmp/f.csv\" (header=true) COLUMNS a:int,b:str PRINT\n",
    "LOAD \"/tmp/f.json\" PRINT\n",
    "LOAD \"/tmp/f.json\" COLUMNS a:int,b:str PRINT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "json, csv, and parquet are support file formats. There are plans to support avro. Notice that parameters can be passed. If running on the default [execution engine](../execution_engine.ipynb), these would be passed on to **Pandas** `read_csv` and `to_csv`.  The file extension is used as a hint to use the appropriate load/save function. If the extension is not present in the filename, it has to be specified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable Assignment\n",
    "\n",
    "From here, it should be getting clear that `Fugue` extends SQL in order to make it a more complete language. One of the additional features is variable assignment. Along with this, multiple `SELECT` statements can be used. This is the equivalent of temp tables or Common Table Expressions (CTE) in SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"number\":[0,1],\"word\":[\"hello\",\"world\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%fsql\n",
    "SELECT * FROM df\n",
    "SAVE OVERWRITE \"/tmp/f.csv\"(header=true)\n",
    "\n",
    "a = LOAD \"/tmp/f.csv\" (header=true)\n",
    "temp = SELECT * FROM a WHERE number=1\n",
    "output = SELECT word FROM temp\n",
    "SAVE OVERWRITE \"/tmp/output.csv\"(header=true)\n",
    "\n",
    "new_a = LOAD \"/tmp/output.csv\"(header=true)\n",
    "PRINT new_a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution Engine\n",
    "\n",
    "So far, we've only dealt with the default [execution engine](../execution_engine.ipynb). If nothing is passed to the `%%fsql`, the `NativeExecutionEngine` is used. Similar to `Fugue` programming interface, the `execution engine` can easily be changed by passing it to `FugueSQLWorkflow`. Below is an example for Spark.\n",
    "\n",
    "Take note of the output `DataFrame` in the example below. It will be a `SparkDataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%fsql spark\n",
    "SELECT *, 1 AS one \n",
    "FROM df\n",
    "PRINT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anonymity\n",
    "\n",
    "In `fugue-sql`, one of the simplifications is anonymity. Itâ€™s optional, but it usually can significantly simplify your code and make it more readable.\n",
    "\n",
    "For a statement that only needs to consume the previous dataframe, a `FROM` keyword is not needed. `PRINT` is the best example. `SAVE` is another example. This is can be applied to other keywords. In this example we'll use the `TAKE` function that just returns the number of rows specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%fsql\n",
    "a = SELECT * FROM df\n",
    "TAKE 2 ROWS PRESORT number DESC          # a is consumed by TAKE\n",
    "PRINT \n",
    "b = SELECT * FROM df\n",
    "TAKE 2 ROWS FROM b PRESORT number DESC   # equivalent explicit synax\n",
    "PRINT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inline Statements\n",
    "\n",
    "The last enchancement is inline statements. One statement can be written in another in between `(` `)` . Anonymity and variable assignment often make this unneeded, but it's just good to know that this option exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%fsql\n",
    "a = CREATE [[0,\"hello\"], [1,\"world\"]] SCHEMA number:int,word:str\n",
    "SELECT *\n",
    "FROM (TAKE 1 ROW FROM a)\n",
    "PRINT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passing DataFrames through fugue-sql cells\n",
    "\n",
    "DataFrames in precending `fugue-sql` cells can not be used in future `fugue-sql` cells by default. In order to use them in downstream cells, the DataFrame needs to be yielded with `YIELD DATAFRAME` like in the example below. This also makes it available in Python cells. For large DataFrames, `YIELD FILE` stores the file in a temporary location for it to be loaded when used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%fsql\n",
    "a=CREATE [[0,\"hello\"],[1,\"world\"]] SCHEMA number:int,word:str\n",
    "YIELD DATAFRAME AS a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the yielded DataFrame in Python\n",
    "print(a.as_pandas().head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%fsql\n",
    "b = CREATE [[0,\"hello2\"],[1,\"world2\"]] SCHEMA number:int,word2:str\n",
    "\n",
    "SELECT a.number num, b.word2 \n",
    "FROM a \n",
    "INNER JOIN b\n",
    "ON a.number = b.number\n",
    "PRINT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From notebooks to deployment\n",
    "\n",
    "While notebooks are good for data exploration and prototyping, some users want to include their `fugue-sql` code in Python scripts. For this, users can use the `fsql` class. Similar to `%%fsql` cells, the execution engine can be defined in the `run` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fugue_sql import fsql\n",
    "\n",
    "fsql(\"\"\"\n",
    "b = CREATE [[0,\"hello2\"],[1,\"world2\"]] SCHEMA number:int,word2:str\n",
    "\n",
    "SELECT a.number num, b.word2 \n",
    "FROM a \n",
    "INNER JOIN b\n",
    "ON a.number = b.number\n",
    "PRINT\n",
    "\"\"\").run(\"spark\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we have gone through how to use standard SQL operations (and more) on top of Pandas, Spark, and Dask. We have also seen enhancements over standard SQL like anonymity and variable assignment.\n",
    "\n",
    "In a [following section](python.ipynb) we'll look at more ways of integrating Python with `fugue-sql` to extend the capabilities of using SQL."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit ('tutorials': conda)",
   "metadata": {
    "interpreter": {
     "hash": "6b529313a359bc0cc3bba050cbf6181d0fbbbf3114ea1d3a405a476853de2242"
    }
   },
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}