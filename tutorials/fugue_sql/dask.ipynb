{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('fugue-examples': conda)",
   "metadata": {
    "interpreter": {
     "hash": "1a800d79b766a92155fd787979cdccc8e97e53dc09ace21dec9020317fb76fdb"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# FugueSQL and Dask-SQL\n",
    "\n",
    "**Pandas** and **Spark** already have solutions that allow users to execute SQL code to describe computation workflows. **Dask**, on the other hand, does not have a standard SQL interface yet. `FugueSQL` provides this feature with the DaskExecutionEngine, but users should also be aware that [dask-sql](https://dask-sql.readthedocs.io/en/latest/index.html) is a relatively new project and has a majority of SQL keywords implemented already. Additionally, it is also faster than FugueSQL on average. However, there are still some features under development. Most notably, the SQL `WINDOW` is not yet implemented.\n",
    "\n",
    "We are collaborating to have our solutions converge to create the de facto SQL interface for Dask. In the meantime, we have unified our solutions by allowing `FugueSQL` to use [dask-sql](https://dask-sql.readthedocs.io/en/latest/index.html) as an [execution engine](../execution_engine.ipynb). The [dask_sql](https://github.com/nils-braun/dask-sql) project has added a `DaskSQLExecutionEngine` into their code to let us import it and pass it into `FugueSQLWorkflow`. Note this is a different engine from `Fugue's DaskExecutionEngine`"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Sample Usage\n",
    "\n",
    "This example below shows that when the SQL keywords are unavailble in `dask-sql`, it will use the `FugueSQL` keywords. We are able to use the `TAKE` and `PRINT` keywords even if they don't exist in `dask-sql`. We can also use the `TRANSFORM and PREPARTITION` even if these are `Fugue` keywords.\n",
    "\n",
    "`FugueSQL` and `dask-sql` together can provide a more powerful solution. This allows us to use both solutions to get the best of both worlds in terms of speed and operation completeness. All we need to do is pass the `DaskSQLExecutionEngine` into `FugueSQLWorkflow`.\n",
    "\n",
    "NOTE: In order for the code below to run, `dask-sql` needs to be installed.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_sql.integrations.fugue import DaskSQLExecutionEngine\n",
    "from fugue_sql import FugueSQLWorkflow\n",
    "import pandas as pd\n",
    "\n",
    "data = [\n",
    "    [\"A\", \"2020-01-01\", 10],\n",
    "    [\"A\", \"2020-01-02\", 20],\n",
    "    [\"A\", \"2020-01-03\", 30],\n",
    "    [\"B\", \"2020-01-01\", 20],\n",
    "    [\"B\", \"2020-01-02\", 30],\n",
    "    [\"B\", \"2020-01-03\", 40]\n",
    "]\n",
    "schema = \"id:str,date:date,value:int\"\n",
    "\n",
    "# schema: *, cumsum:int\n",
    "def cumsum(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df[\"cumsum\"] = df['value'].cumsum()\n",
    "    return df\n",
    "\n",
    "# Run the DAG on the DaskSQLExecutionEngine by dask-sql\n",
    "with FugueSQLWorkflow(DaskSQLExecutionEngine) as dag:\n",
    "    df = dag.df(data, schema)\n",
    "    dag(\"\"\"\n",
    "    SELECT *\n",
    "    FROM df\n",
    "    TRANSFORM PREPARTITION BY id PRESORT date ASC USING cumsum\n",
    "    TAKE 5 ROWS\n",
    "    PRINT\n",
    "    \"\"\")"
   ]
  },
  {
   "source": [
    "When a SQL keywords don't exist in `dask-sql`, it will default to the `Fugue DaskExecutionEngine`. However, when the keyword is registered by `dask-sql` it will use their implementation. `OVER PARITION` is registered but still being developed, which will cause errors. One workaround is to use `Fugue's TRANSFORM and PREPARTITION` like above to avoid using `OVER PARTITION` for now."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "---\n",
    "**Conflict with SparkExecutionEngine**\n",
    "\n",
    "Note that `dask-sql` requires Python 3.8 to run, which may cause errors with the SparkExecutionEngine because Spark is more stable on Python 3.7. \n",
    "\n",
    "---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}