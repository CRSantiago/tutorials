{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('fugue-examples': conda)",
   "metadata": {
    "interpreter": {
     "hash": "1a800d79b766a92155fd787979cdccc8e97e53dc09ace21dec9020317fb76fdb"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# FugueSQL and Dask-SQL\n",
    "\n",
    "Pandas and Spark have solutions that allow users to execute SQL code to describe computation workflows. Dask, on the other hand, has not had a standard SQL interface, until recently. [`dask-sql`](https://dask-sql.readthedocs.io/en/latest/index.html) is a relatively new project and has a majority of SQL keywords implemented already. Additionally, it is also faster than FugueSQL on average. However, there are still some features under development. Most notably, the SQL `WINDOW` is not yet implemented.\n",
    "\n",
    "We are collaborating to have our solutions converge to create the de facto SQL interface for Dask. In the meantime, we have unified our solutions by allowing `Fugue` to use `dask-sql` as a SQL (execution engine)[../execution_engine.ipynb] for `FugueSQLWorkflow`. `dask-sql` has added code that lets us import it and pass it into `FugueSQLWorkflow`."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Sample Usage\n",
    "\n",
    "This example below shows that when the keywords are unavailble in `dask-sql`, it will use the FugueSQL keywords. We are able to use the `RANK()` function, which is an example of a `WINDOW` function along with the `FugueSQL` `TAKE` keyword.\n",
    "\n",
    "`FugueSQL` and `dask-sql` together can provide a more powerful solution. This allows us to use both solutions to get the best of both worlds in terms of speed and completeness. All we need to do is pass the `DaskSQLExecutionEngine` into `FugueSQLWorkflow`\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_sql.integrations.fugue import DaskSQLExecutionEngine\n",
    "from fugue_sql import FugueSQLWorkflow\n",
    "\n",
    "data = [\n",
    "    [\"A\", \"2020-01-01\", 10],\n",
    "    [\"A\", \"2020-01-02\", None],\n",
    "    [\"A\", \"2020-01-03\", 30],\n",
    "    [\"B\", \"2020-01-01\", 20],\n",
    "    [\"B\", \"2020-01-02\", None],\n",
    "    [\"B\", \"2020-01-03\", 40]\n",
    "]\n",
    "schema = \"id:str,date:date,value:double\"\n",
    "\n",
    "with FugueSQLWorkflow(DaskSQLExecutionEngine) as dag:\n",
    "    df = dag.df(data, schema)\n",
    "    dag(\"\"\"\n",
    "\n",
    "    SELECT id, date, value,\n",
    "    RANK() OVER (PARTITION BY id ORDER BY date) row\n",
    "    FROM df\n",
    "    TAKE 2 ROWS PREPARTITION BY id PRESORT value NULLS FIRST\n",
    "    PRINT\n",
    "    \"\"\")"
   ]
  },
  {
   "source": [
    "---\n",
    "**Conflict with SparkExecutionEngine**\n",
    "\n",
    "Note that `dask-sql` requires Python 3.8 to run, which may cause errors with the SparkExecutionEngine because Spark is more stable on Python 3.7. \n",
    "\n",
    "---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}