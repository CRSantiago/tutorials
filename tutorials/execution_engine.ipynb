{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execution Engine\n",
    "\n",
    "It is the heart of Fugue. It is the layer that unifies core concepts of distributed computing, and separates the underlying computing frameworks from user's higher level logic. Normally you don't directly operate on execution engines. But it's good to understand some basics.\n",
    "\n",
    "In Fugue, **the only dataset is schemaed dataframes**, so although there are other important concepts such as `RDD`, we don't touch them. More options may result in more flexibility or more confusion. You can read these [1](https://databricks.com/blog/2016/07/14/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html) [2](https://data-flair.training/blogs/apache-spark-rdd-vs-dataframe-vs-dataset/) to get more insights.\n",
    "\n",
    "However, it is important to understand that, you have full access to any underlying computing frameworks, and to use any specific features including `RDD`. We unify certain things to make them easy and consistent, but we don't block anything else.\n",
    "\n",
    "## Initialization\n",
    "\n",
    "Although there is no hard rules for initializing an ExecutionEngine. The general way is to initialize from configs. We should use the config to describe each components, such as what logger to use, what SQLEngine to use and other properties.\n",
    "\n",
    "Here is the best practice to initialize each built-in ExecutionEngine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fugue.execution import NativeExecutionEngine\n",
    "\n",
    "engine = NativeExecutionEngine({\"myconfig\":\"abc\"})\n",
    "assert engine.conf.get_or_throw(\"myconfig\" ,str) == \"abc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from distributed import Client\n",
    "client = Client() # without this, dask is not in distributed mode\n",
    "from fugue_dask.execution_engine import DaskExecutionEngine\n",
    "\n",
    "# fugue.dask.dataframe.default.partitions determines the default partitions for a new DaskDataFrame\n",
    "engine = DaskExecutionEngine({\"fugue.dask.dataframe.default.partitions\":4})\n",
    "assert engine.conf.get_or_throw(\"fugue.dask.dataframe.default.partitions\" ,int) == 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from fugue_spark.execution_engine import SparkExecutionEngine\n",
    "\n",
    "# here is the place you get a spark session, it's the same as if there is no Fugue\n",
    "# notice that you can configure almost everything in this way, even running mode, such as local mode or other mode\n",
    "# the best way based on my experience is to only use this way + spark-defaults.conf to initialize SparkSessions.\n",
    "spark_session = (SparkSession\n",
    "                 .builder\n",
    "                 .config(\"spark.executor.cores\",4)\n",
    "                 .config(\"fugue.dummy\",\"dummy\")\n",
    "                 .getOrCreate())\n",
    "\n",
    "engine = SparkExecutionEngine(spark_session, {\"additional_conf\":\"abc\"})\n",
    "assert engine.conf.get_or_throw(\"spark.executor.cores\" ,int) == 4\n",
    "assert engine.conf.get_or_throw(\"fugue.dummy\" ,str) == \"dummy\"\n",
    "assert engine.conf.get_or_throw(\"additional_conf\" ,str) == \"abc\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A special feature of Fugue is that, the `engine.conf` is also accessible on workers within all types of Fugue exetensions. The engine itself is only accessible on driver or dirver side extensions.\n",
    "\n",
    "\n",
    "## Create DataFrame\n",
    "With ExecutionEngine, you only need to tell the system I need to create a DataFrame with certain raw data or data source. And with different ExecutionEngines, different types of DataFrames will be created. `to_df` is the common interface for ExecutionEngines, use it to create DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'fugue.dataframe.array_dataframe.ArrayDataFrame'>\n",
      "<class 'fugue_spark.dataframe.SparkDataFrame'>\n"
     ]
    }
   ],
   "source": [
    "from fugue.execution import NativeExecutionEngine\n",
    "from fugue_spark.execution_engine import SparkExecutionEngine\n",
    "\n",
    "engine1 = NativeExecutionEngine()\n",
    "engine2 = SparkExecutionEngine() # if spark_session is not provided, it will get the current active session\n",
    "\n",
    "df1 = engine1.to_df([[0]],\"a:int\")\n",
    "df2 = engine2.to_df([[0]],\"a:int\")\n",
    "\n",
    "print(type(df1))\n",
    "print(type(df2))\n",
    "assert df1.as_array() == df2.as_array() # both materialized on driver, and compare"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
