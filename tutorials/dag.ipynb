{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execution Graph (DAG) & Programming Interface\n",
    "\n",
    "This is the most important tutorial you need to read through.\n",
    "\n",
    "The construction of workflow and the construction of ExecutionEngines can be totally decoupled. But you can also couple them, this may be useful when you need to use certain config in an ExecutionEngine. \n",
    "\n",
    "In fugue, workflow is static, it is strictly the description or **spec** of your logic flow, and it has nothing to do with execution. When you finish writing the workflow spec, you can also choose a Fugue ExecutionEngine and an Adagio workflow ExecutionEngine for the end to end execution. The Adagio workflow ExecutionEngine will turn your workflow _spec_ into a workflow _instance_ (nodes specs will also turn to node instances) and execute in certain order. For example Adagio default ExecutionEngine `SequentialExecutionEngine` will run everything sequentially in the order you list in the workflow spec. In each node of the execution graph, it will use the given Fugue ExecutionEngine. And the edges of the graph is always `DataFrame`\n",
    "\n",
    "\n",
    "## Graph Nodes - Extensions\n",
    "\n",
    "There are only 3 types of nodes in Fugue workflow. They are also called driver side extensions.\n",
    "\n",
    "<img src=\"../images/nodes.svg\" width=\"500\">\n",
    "\n",
    "* **Creator**: no input, single output dataframe, it is to produce dataframe input for other types of nodes, for example load file or create mock data\n",
    "* **Processor**: one or multiple input dataframes, single output dataframe, it is to do certain transformation and pass to the next node\n",
    "* **Outputter**: one or multiple input dataframes, no input, it is to finalize the process of the input, for example save or print\n",
    "\n",
    "All these nodes/extensions are executed on driver side and they are ExecutionEngine aware. For example if you really want to use certain features of RDD, you can write your native Spark code in a `Processor` because inside a processor, you can access the ExecutionEngine and if it is SparkExecutionEngine, you can get `spark_session`. There will be examples in this tutorial.\n",
    "\n",
    "There are two special types of `Processor`: `Transformer` and `CoTransformer`. They are special because they are NOT ExeuctionEngine aware, and their exposed interface is to describe the job to do on worker side not driver side.\n",
    "\n",
    "<img src=\"../images/transformers.svg\" width=\"300\">\n",
    "\n",
    "* **Transformer**: single `LocalDataFrame` in, single `LocalDataFrame` out\n",
    "* **CoTransformer**: one or multiple `LocalDataFrame` in, single `LocaDataFrame` out\n",
    "\n",
    "They only care about how to process a partition of the dataframe(s) on a local machine.\n",
    "\n",
    "| . | Creator | Processor | Outputter | Transformer | CoTransformer\n",
    "|---|---|---|---|---|---\n",
    "|Input | 0    | 1+        | 1+        | 1           | 1+\n",
    "|Output| 1    | 1         | 0         | 1           | 1\n",
    "|Side  |Driver|Driver     | Driver    | Worker      | Worker\n",
    "|Engine Aware | Yes | Yes | Yes       | No          | No\n",
    "\n",
    "A Fugue Workflow consists of these extensions, we orchestrate the extensions.\n",
    "\n",
    "\n",
    "## Initialize a Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fugue.workflow import FugueWorkflow\n",
    "\n",
    "dag = FugueWorkflow()\n",
    "df = dag.df([[0]],\"a:int\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, nothing printed, this is because you only described what you want to do but you didn't really execute the dag. To run it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dag.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to make it run automatically, use `with` statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with FugueWorkflow() as dag:\n",
    "    df = dag.df([[0]],\"a:int\")\n",
    "    df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, where did I setup the ExecutionEngine? So actually by default, `FugueWorkflow` will use `NativeExecutionEngine`, in order to setup your own ExecutionEngine, you can do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from fugue_spark.execution_engine import SparkExecutionEngine\n",
    "\n",
    "\n",
    "dag = FugueWorkflow()\n",
    "df = dag.df([[0]],\"a:int\")\n",
    "df.show()\n",
    "\n",
    "# see all the expensive initialization is after the dag is constructed, the steps above are fast\n",
    "# if you have any obvious issue, they will fail fast, \n",
    "# they are compile time problems and the below is all about runtime\n",
    "spark_session = (SparkSession\n",
    "                 .builder\n",
    "                 .config(\"spark.executor.cores\",4)\n",
    "                 .getOrCreate())\n",
    "\n",
    "my_engine = SparkExecutionEngine(spark_session)\n",
    "dag.run(my_engine)\n",
    "\n",
    "# You can also do the following, but you move the dag construction to be after building spark session\n",
    "# so at least you have to wait until the spark session is initialized, you can know if there is compile problems\n",
    "with FugueWorkflow(my_engine) as dag:\n",
    "    df = dag.df([[0]],\"a:int\")\n",
    "    df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fugue.workflow import FugueWorkflow\n",
    "import pandas as pd\n",
    "from fugue.dataframe import ArrayDataFrame\n",
    "\n",
    "dag = FugueWorkflow()\n",
    "# You can pass in raw data, pandas dataframe or Fugue DataFrames\n",
    "dag.df([[0]],\"a:int\").show()\n",
    "dag.df(pd.DataFrame([[0]], columns=[\"a\"])).show()\n",
    "dag.df(ArrayDataFrame([[0]],\"a:int\")).show()\n",
    "dag.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
