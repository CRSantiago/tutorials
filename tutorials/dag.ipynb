{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execution Graph (DAG) & Programming Interface\n",
    "\n",
    "This is the most important tutorial you need to read through.\n",
    "\n",
    "The construction of workflow and the construction of ExecutionEngines can be totally decoupled. But you can also couple them, this may be useful when you need to use certain config in an ExecutionEngine. \n",
    "\n",
    "In fugue, workflow is static, it is strictly the description or **spec** of your logic flow, and it has nothing to do with execution. When you finish writing the workflow spec, you can also choose a Fugue ExecutionEngine and an Adagio workflow ExecutionEngine for the end to end execution. The Adagio workflow ExecutionEngine will turn your workflow _spec_ into a workflow _instance_ (nodes specs will also turn to node instances) and execute in certain order. For example Adagio default ExecutionEngine `SequentialExecutionEngine` will run everything sequentially in the order you list in the workflow spec. In each node of the execution graph, it will use the given Fugue ExecutionEngine. And the edges of the graph is always `DataFrame`\n",
    "\n",
    "\n",
    "## Graph Nodes - Extensions\n",
    "\n",
    "There are only 3 types of nodes in Fugue workflow. They are also called driver side extensions.\n",
    "\n",
    "<img src=\"../images/nodes.svg\" width=\"500\">\n",
    "\n",
    "* **Creator**: no input, single output dataframe, it is to produce dataframe input for other types of nodes, for example load file or create mock data\n",
    "* **Processor**: one or multiple input dataframes, single output dataframe, it is to do certain transformation and pass to the next node\n",
    "* **Outputter**: one or multiple input dataframes, no input, it is to finalize the process of the input, for example save or print\n",
    "\n",
    "All these nodes/extensions are executed on driver side and they are ExecutionEngine aware. For example if you really want to use certain features of RDD, you can write your native Spark code in a `Processor` because inside a processor, you can access the ExecutionEngine and if it is SparkExecutionEngine, you can get `spark_session`. There will be examples in this tutorial.\n",
    "\n",
    "There are two special types of `Processor`: `Transformer` and `CoTransformer`. They are special because they are NOT ExeuctionEngine aware, and their exposed interface is to describe the job to do on worker side not driver side.\n",
    "\n",
    "<img src=\"../images/transformers.svg\" width=\"300\">\n",
    "\n",
    "* **Transformer**: single `LocalDataFrame` in, single `LocalDataFrame` out\n",
    "* **CoTransformer**: one or multiple `LocalDataFrame` in, single `LocaDataFrame` out\n",
    "\n",
    "They only care about how to process a partition of the dataframe(s) on a local machine.\n",
    "\n",
    "| . | Creator | Processor | Outputter | Transformer | CoTransformer\n",
    "|---|---|---|---|---|---\n",
    "|Input | 0    | 1+        | 1+        | 1           | 1+\n",
    "|Output| 1    | 1         | 0         | 1           | 1\n",
    "|Side  |Driver|Driver     | Driver    | Worker      | Worker\n",
    "|Engine Aware | Yes | Yes | Yes       | No          | No\n",
    "\n",
    "A Fugue Workflow consists of these extensions, we orchestrate the extensions.\n",
    "\n",
    "\n",
    "## Initialize a Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fugue.workflow import FugueWorkflow\n",
    "\n",
    "dag = FugueWorkflow()\n",
    "df = dag.df([[0]],\"a:int\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, nothing printed, this is because you only described what you want to do but you didn't really execute the dag. To run it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dag.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to make it run automatically, use `with` statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with FugueWorkflow() as dag:\n",
    "    df = dag.df([[0]],\"a:int\")\n",
    "    df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, where did I setup the ExecutionEngine? So actually by default, `FugueWorkflow` will use `NativeExecutionEngine`, in order to setup your own ExecutionEngine, you can do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from fugue_spark.execution_engine import SparkExecutionEngine\n",
    "\n",
    "\n",
    "dag = FugueWorkflow()\n",
    "df = dag.df([[0]],\"a:int\")\n",
    "df.show()\n",
    "\n",
    "# see all the expensive initialization is after the dag is constructed, the steps above are fast\n",
    "# if you have any obvious issue, they will fail fast, \n",
    "# they are compile time problems and the below is all about runtime\n",
    "spark_session = (SparkSession\n",
    "                 .builder\n",
    "                 .config(\"spark.executor.cores\",4)\n",
    "                 .getOrCreate())\n",
    "\n",
    "my_engine = SparkExecutionEngine(spark_session)\n",
    "dag.run(my_engine)\n",
    "\n",
    "# You can also do the following, but you move the dag construction to be after building spark session\n",
    "# so at least you have to wait until the spark session is initialized, you can know if there is compile problems\n",
    "with FugueWorkflow(my_engine) as dag:\n",
    "    df = dag.df([[0]],\"a:int\")\n",
    "    df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fugue.workflow import FugueWorkflow\n",
    "import pandas as pd\n",
    "from fugue.dataframe import ArrayDataFrame\n",
    "\n",
    "dag = FugueWorkflow()\n",
    "# You can pass in raw data, pandas dataframe or Fugue DataFrames\n",
    "dag.df([[0]],\"a:int\").show()\n",
    "dag.df(pd.DataFrame([[0]], columns=[\"a\"])).show()\n",
    "dag.df(ArrayDataFrame([[0]],\"a:int\")).show()\n",
    "dag.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create DataFrame with Creator\n",
    "\n",
    "Please read [creator tutorial](./creator.ipynb) for details how to write creators in different ways. Here is just one way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fugue.workflow import FugueWorkflow\n",
    "from typing import List, Any\n",
    "\n",
    "#schema: a:int\n",
    "def create_single(n=1) -> List[List[Any]]:\n",
    "    return [[n]]\n",
    "\n",
    "with FugueWorkflow() as dag:\n",
    "    dag.create(create_single).show()\n",
    "    dag.create(create_single, params={\"n\":2}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process DataFrame with Processor\n",
    "\n",
    "Please read [processor tutorial](./processor.ipynb) for details how to write processor in different ways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fugue.workflow import FugueWorkflow\n",
    "from fugue.execution import ExecutionEngine\n",
    "from fugue.dataframe import DataFrame, ArrayDataFrame\n",
    "from typing import List, Any\n",
    "\n",
    "def ct(e:ExecutionEngine, df1:DataFrame, df2:DataFrame) -> DataFrame:\n",
    "    # if the function signature has ExecutionEngine, the creator becomes engine aware\n",
    "    # here we need engine to persist the dataframes before counting\n",
    "    c1, c2 = e.persist(df1).count(), e.persist(df2).count()\n",
    "    return ArrayDataFrame([[c1,c2]],\"c1:long,c2:long\")\n",
    "\n",
    "with FugueWorkflow() as dag:\n",
    "    a=dag.df([[0]],\"a:int\")\n",
    "    b=dag.df([[1],[3]],\"a:int\")\n",
    "    dag.process(a,b,using=ct).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output DataFrame with Outputter\n",
    "\n",
    "Please read [outputter tutorial](./outputter.ipynb) for details how to write outputter in different ways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fugue.workflow import FugueWorkflow\n",
    "from fugue.dataframe import DataFrame, ArrayDataFrame, DataFrames\n",
    "from typing import List, Any\n",
    "\n",
    "def print_array(dfs:DataFrames) -> None:\n",
    "    for df in dfs.values():\n",
    "        print(df.as_array())\n",
    "    print(\"---\")\n",
    "\n",
    "with FugueWorkflow() as dag:\n",
    "    a=dag.df([[0]],\"a:int\")\n",
    "    b=dag.df([[1],[3]],\"a:int\")\n",
    "    dag.output(a,b,using=print_array)\n",
    "    # you can also directly call output from a dag dataframe\n",
    "    a.output(print_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform & CoTransform\n",
    "\n",
    "Please read [transformer tutorial](./transformer.ipynb) and [cotransformer tutorial](./cotransformer.ipynb) for details how to write transformers in different ways\n",
    "\n",
    "In this example we want to get nth smallest or largest row of each partition of column `a`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fugue.workflow import FugueWorkflow\n",
    "from fugue.dataframe import DataFrame, ArrayDataFrame, DataFrames\n",
    "from fugue_spark.execution_engine import SparkExecutionEngine\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from timeit import timeit\n",
    "from typing import Iterable, List, Any\n",
    "\n",
    "def helper(ct=20000) -> pd.DataFrame:\n",
    "    return pd.DataFrame(np.random.randint(0,100,size=(ct, 3)), columns=list('abc'))\n",
    "\n",
    "# schema: *\n",
    "def nth(df:Iterable[List[Any]], n) -> Iterable[List[Any]]:\n",
    "    for row in df:\n",
    "        if n==0:\n",
    "            yield row\n",
    "            return\n",
    "        n-=1\n",
    "        \n",
    "dag = FugueWorkflow()\n",
    "df = dag.create(helper).persist() # add persist here so the data will not be regenerated\n",
    "largest_4 = df.partition(by=[\"a\"],presort=\"b DESC, c DESC\").transform(nth, params={\"n\":4})\n",
    "smallest_4 = df.partition(by=[\"a\"],presort=\"b,c\").transform(nth, params={\"n\":4})\n",
    "largest_4.persist().show()\n",
    "smallest_4.persist().show()\n",
    "\n",
    "print(timeit(lambda: dag.run(), number=1)) # using native python\n",
    "\n",
    "engine = SparkExecutionEngine()\n",
    "print(timeit(lambda: dag.run(engine), number=1)) # if engine is well configured, this logic can handle very large dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, on such small data, Spark local mode is way slower than running on `NativeExecutionEngine`. In practice, when we unit test Fugue code, try to use `NativeExecutionEngine` since their behavior should be consistent. This is guaranteed on Fugue level, you only choose the faster one in a certain scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fugue.workflow import FugueWorkflow\n",
    "from fugue.dataframe import DataFrame, ArrayDataFrame, DataFrames\n",
    "from fugue_spark.execution_engine import SparkExecutionEngine\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from timeit import timeit\n",
    "from typing import Iterable, List, Any\n",
    "\n",
    "def helper(ct=20) -> pd.DataFrame:\n",
    "    np.random.seed(0)\n",
    "    return pd.DataFrame(np.random.randint(0,4,size=(ct, 3)), columns=list('abc'))\n",
    "\n",
    "# schema: value:[str]\n",
    "def to_str(df1:List[List[Any]], df2:List[List[Any]]) -> List[List[Any]]:\n",
    "    return [[[df1.__repr__(),df2.__repr__()]]]\n",
    "        \n",
    "dag = FugueWorkflow()\n",
    "df1 = dag.create(helper)\n",
    "df2 = dag.create(helper)\n",
    "df1.zip(df2,how=\"inner\",partition={\"by\":[\"a\"],\"presort\":\"b,c\"}).transform(to_str).show()\n",
    "\n",
    "print(timeit(lambda: dag.run(), number=1))\n",
    "\n",
    "engine = SparkExecutionEngine()\n",
    "print(timeit(lambda: dag.run(engine), number=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
