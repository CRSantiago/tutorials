{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Callbacks From Transformers To Driver\n",
    "\n",
    "Transformers are the only Fugue extensions to execute on remote worker nodes. For some scenarios, the transformers need to communicate with the driver while it is running. For example, a transformer is training a Keras model, and at the end of each epoch, it needs to report the metrics to the driver and the driver can respond with a decision whether to stop the training.\n",
    "\n",
    "In Fugue, the callback model is also abstract, you only need to define the callback functions and specify the server to handle remote calls.\n",
    "\n",
    "## The simplest example\n",
    "\n",
    "The simplest way to have a call back, is to define a callback parameter in the interfaceless way. You only need to annotate the parameter with `callable`, `Callable` or `Callable` with arguments, for example `Callable[[str],str]`. And this parameter must be after all dataframe parameters and before all other parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from fugue import FugueWorkflow\n",
    "\n",
    "# schema: *\n",
    "def print_describe_and_return(df:pd.DataFrame, cb:callable) -> pd.DataFrame:\n",
    "    cb(str(df.describe()))\n",
    "    return df\n",
    "\n",
    "dag = FugueWorkflow()\n",
    "df = dag.df([[0,0],[1,1],[0,1],[2,2]],\"a:int,b:int\")\n",
    "df.partition(by=[\"a\"]).transform(print_describe_and_return, callback = lambda x:print(x)).show()\n",
    "\n",
    "dag.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example, it's a typical interfaceless transformer example with two additions: `cb:callback` in the transformer, and `callback = lambda x:print(x)` in the transform function. `cb:callback` is to tell Fugue that we want to use a callback inside the transformation. `callback = lambda x:print(x)` is to define the function that will execute on the driver.\n",
    "\n",
    "As you can see, since there are 3 partitions of `a`, there are 3 descriptions printed, and in the end, the output dataframe is also printed.\n",
    "\n",
    "## Callbacks on distributed execution engines\n",
    "\n",
    "The above code is running using `NativeExecutionEngine` running on the current process. It's the minimal code to test whether your callback logic will work. To run it using a distributed engine, You need to setup the callback server to handle network calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fugue_spark import SparkExecutionEngine\n",
    "\n",
    "conf = {\n",
    "    \"fugue.rpc.server\": \"fugue.rpc.flask.FlaskRPCServer\",\n",
    "    \"fugue.rpc.flask_server.host\": \"0.0.0.0\",\n",
    "    \"fugue.rpc.flask_server.port\": \"1234\",\n",
    "    \"fugue.rpc.flask_server.timeout\": \"2 sec\",\n",
    "}\n",
    "\n",
    "dag.run(SparkExecutionEngine(conf=conf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code uses the built-in flask server to handle network calls from workers. To use `fugue.rpc.flask.FlaskRPCServer`, you must set `fugue.rpc.flask_server.host` and `fugue.rpc.flask_server.port`, and it's suggested to also set `fugue.rpc.flask_server.timeout` to a reasonable timeout for your own case.\n",
    "\n",
    "You can also create your custom server by implementing [RPCServer](https://fugue.readthedocs.io/en/latest/api/fugue.rpc.html#fugue.rpc.base.RPCServer) and [RPCClient](https://fugue.readthedocs.io/en/latest/api/fugue.rpc.html#fugue.rpc.base.RPCClient). For example you may create a pair of server and client to communicate with [MLFlow](https://mlflow.org/) to update metrics in realtime.\n",
    "\n",
    "\n",
    "## Stateful callbacks\n",
    "\n",
    "Commonly, callbacks need to be stateful. In Fugue, it's totally fine to set the callback to be a method of an instance (in order to be stateful), or to use a global method/variable. You only need to make the function thread safe because it could be invoked in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from threading import RLock\n",
    "\n",
    "class Callback(object):\n",
    "    def __init__(self):\n",
    "        self.n=0\n",
    "        self._update_lock = RLock()\n",
    "        \n",
    "    def should_skip(self):\n",
    "        with self._update_lock:\n",
    "            self.n+=1\n",
    "            return self.n>=3\n",
    "                \n",
    "callback = Callback()\n",
    "                \n",
    "                \n",
    "# schema: *\n",
    "def take(df:pd.DataFrame, skip:callable) -> pd.DataFrame:\n",
    "    if not skip():\n",
    "        return df\n",
    "\n",
    "dag = FugueWorkflow()\n",
    "df = dag.df([[0,0],[1,1],[0,1],[2,2]],\"a:int,b:int\")\n",
    "df.partition(by=[\"a\"]).transform(take, callback = callback.should_skip).show()\n",
    "\n",
    "dag.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example, we only take two partitions of the entire dataframe, so the `Callback` implemented a thread safe counter, and return true or false based on the counter.\n",
    "\n",
    "**The only requirement** for a callback function that Fugue can use is that its input parameters and output are picklable (Nones are fine). The function itself is OK if not picklable. In the above case, `should_skip` invoked `RLock` which is not picklable, but it doesn't matter.\n",
    "\n",
    "\n",
    "## Implementing `RPCHandler` instead\n",
    "\n",
    "In most case the above native approaches are sufficient. However, if you want to having more control on the callback side, you can directly implement [RPCHandler](https://fugue.readthedocs.io/en/latest/api/fugue.rpc.html#fugue.rpc.base.RPCHandler). For example, you want to start a thread to process the incoming calls and stop the thread when the execution finishes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from threading import RLock\n",
    "from fugue.rpc import RPCHandler\n",
    "from uuid import uuid4\n",
    "\n",
    "class Callback(RPCHandler):\n",
    "    def __init__(self):\n",
    "        super().__init__()  # <-- must call\n",
    "        self.n=0\n",
    "        self._update_lock = RLock()\n",
    "        \n",
    "    def __uuid__(self) -> str:\n",
    "        \"\"\"UUID that can affect the determinism of the workflow\"\"\"\n",
    "        return str(uuid4())  # in this case, it will make the workflow non deterministic\n",
    "\n",
    "    def start_handler(self) -> None:\n",
    "        \"\"\"User implementation of starting the handler\"\"\"\n",
    "        print(\"counter started\")\n",
    "\n",
    "\n",
    "    def stop_handler(self) -> None:\n",
    "        \"\"\"User implementation of stopping the handler\"\"\"\n",
    "        print(\"counter stopped\")\n",
    "        \n",
    "    def __call__(self):\n",
    "        with self._update_lock:\n",
    "            self.n+=1\n",
    "            return self.n>=3\n",
    "                \n",
    "callback = Callback()\n",
    "                \n",
    "                \n",
    "# schema: *\n",
    "def take(df:pd.DataFrame, skip:callable) -> pd.DataFrame:\n",
    "    if not skip():\n",
    "        return df\n",
    "\n",
    "dag = FugueWorkflow()\n",
    "df = dag.df([[0,0],[1,1],[0,1],[2,2]],\"a:int,b:int\")\n",
    "df.partition(by=[\"a\"]).transform(take, callback = callback).show()\n",
    "\n",
    "print(dag.spec_uuid())  # every time, the id will be different because the Callback is not deterministic\n",
    "dag.run()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using callbacks in Transformer class\n",
    "\n",
    "If you must implement a `Transformer`, `OutputTransformer`, `CoTransformer` and `OutputCoTransformer`, then you can use `rpc_client` as the callback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fugue import FugueWorkflow, Transformer\n",
    "\n",
    "class PrintAndReturn(Transformer):\n",
    "    def get_output_schema(self, df):\n",
    "        return df.schema\n",
    "    \n",
    "    def transform(self, df):\n",
    "        self.rpc_client(str(df.as_pandas().describe()))\n",
    "        return df\n",
    "\n",
    "dag = FugueWorkflow()\n",
    "df = dag.df([[0,0],[1,1],[0,1],[2,2]],\"a:int,b:int\")\n",
    "df.partition(by=[\"a\"]).transform(PrintAndReturn, callback = lambda x:print(x)).show()\n",
    "\n",
    "dag.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A real example: ploting mins in real time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fugue import FugueWorkflow\n",
    "from fugue.rpc import RPCHandler\n",
    "import pandas as pd\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "from threading import RLock, Thread\n",
    "from time import sleep\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class PlotMinNow(RPCHandler):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self._update_lock = RLock()\n",
    "        self._values = []\n",
    "        self._updated = False\n",
    "        self._shutdown = False\n",
    "        self._thread = None\n",
    "        \n",
    "    def __call__(self, value):\n",
    "        with self._update_lock:\n",
    "            if len(self._values)==0 or value<self._values[-1]:\n",
    "                self._values.append(value)\n",
    "                self._updated=True\n",
    "\n",
    "    def start_handler(self):\n",
    "        def thread():\n",
    "            def _plot():\n",
    "                with self._update_lock:\n",
    "                    data = list(self._values) if self._updated else []\n",
    "                if len(data)>0:\n",
    "                    clear_output()\n",
    "                    pd.Series(data).plot()\n",
    "                    plt.show()\n",
    "                    self._updated=False\n",
    "\n",
    "            while not self._shutdown:\n",
    "                _plot()\n",
    "                sleep(1)\n",
    "            _plot()\n",
    "        \n",
    "        self._thread = Thread(target=thread)\n",
    "        self._thread.start()\n",
    "        \n",
    "        \n",
    "    def stop_handler(self):\n",
    "        self._shutdown=True\n",
    "        self._thread.join()\n",
    "        \n",
    "with PlotMinNow().start() as p:\n",
    "    p(10)\n",
    "    p(9.5)\n",
    "    p(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create() -> pd.DataFrame:\n",
    "    return pd.DataFrame([[100-x] for x in range(100)], columns=[\"a\"])\n",
    "\n",
    "def plot(df:pd.DataFrame, p:callable) -> None:\n",
    "    random.seed(0)\n",
    "    for v in df[\"a\"]/100.0:\n",
    "        p(random.random()*v)\n",
    "        sleep(0.2)\n",
    "\n",
    "with FugueWorkflow() as dag:\n",
    "    dag.create(create).out_transform(plot, callback=PlotMinNow())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use with caution\n",
    "\n",
    "Callbacks may be convenient to use, but you should use with caution. For example, it may not be a good idea to direct worker sider logs to driver using this approach because the amount of data can be unexpectedly large.\n",
    "\n",
    "Also, when you implement the driver side logic for the callbacks, you should be careful about the contention and latency. Take a look at the `_plot` in `PlotMinNow`, it's a consumer competing with the data producers (remote callers), so it minimizes the logic inside the locked part to reduce such contention.\n",
    "\n",
    "The CPU usage is also a concern, when multiple workers are calling back, it could overload the system. So you should consider decoupling producers and consumers and moving the expensive operations to the consumer side so that you have better control of the load. See `__call__` in `PlotMinNow`, it has very cheap operations, if we re-draw the chart in side `__call__`, it may be a bad idea."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
